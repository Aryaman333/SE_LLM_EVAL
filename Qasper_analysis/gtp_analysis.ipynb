{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aryam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import jsonlines\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import openai\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "import itertools\n",
    "import string\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "import gensim.downloader as api\n",
    "import sys\n",
    "sys.path.insert(0, './Metrics')\n",
    "from Automatic_Metrics import Scorers\n",
    "from Linguistic_Metrics import LinguisticFeatures\n",
    "import Stylistic_Metrics as st\n",
    "import  Metrics.Stylistic_Metrics\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key =  '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(r\"Data\\qasper-test-v0.3.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "for paper_id, paper_info in data.items():\n",
    "    doc = {}\n",
    "    doc['doc_id'] = paper_id\n",
    "    doc['title'] = paper_info['title']\n",
    "    doc['qas'] = []\n",
    "    for qa in paper_info['qas']:\n",
    "        answers = []\n",
    "        evidence = \" \"\n",
    "\n",
    "        for ans in qa['answers']:\n",
    "            ext_spans_combined = \" \"\n",
    "            extractive_spans = ans['answer']['extractive_spans']\n",
    "            if len(extractive_spans)>1:\n",
    "                for es in extractive_spans:\n",
    "                    ext_spans_combined =ext_spans_combined+\" \"+es\n",
    "            free_form_answer = ans['answer']['free_form_answer']\n",
    "            ev = ans['answer']['evidence']\n",
    "            for e in ev:\n",
    "                if e not in evidence:\n",
    "                    evidence=evidence+\" \"+e\n",
    "            if extractive_spans and extractive_spans not in answers:\n",
    "                answers.append(extractive_spans)\n",
    "            if free_form_answer and not extractive_spans and free_form_answer not in answers :\n",
    "                answers.append(free_form_answer)\n",
    "            if ext_spans_combined != \" \" and ext_spans_combined not in answers :\n",
    "                answers.append(ext_spans_combined)\n",
    "        if answers:\n",
    "            qa_copy = qa.copy()\n",
    "            qa_copy['answers'] = answers\n",
    "            qa_copy['evidence'] = evidence\n",
    "            doc['qas'].append(qa_copy)\n",
    "    clean_data.append(doc)\n",
    "save_json(clean_data, r'Data\\data_cleaned.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = json.load(open(r\"Data\\data_cleaned.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(lst):\n",
    "    flattened = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            flattened.extend(flatten_list(item))\n",
    "        else:\n",
    "            flattened.append(item)\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,datum in enumerate(clean_data):\n",
    "    for qa in datum['qas']:\n",
    "        flattened_list = flatten_list(qa['answers'])\n",
    "        qa['answers'] = flattened_list\n",
    "save_json(clean_data, r'Data\\data_preprocessed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = json.load(open(r\"Data\\data_preprocessed.json\"))\n",
    "data_reduced = clean_data[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    completions = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature = 0.0,\n",
    "        messages=messages)\n",
    "    gpt_response = completions['choices'][0]['message']['content'].strip() \n",
    "    return gpt_response   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prompt_1(contents,question):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a NLP domain expert question answering system.\n",
    "                The user will provide you with the contents of a research paper.\n",
    "                The user will also provide a question to be answered.\n",
    "                Strictly answer in one sentence or less. \n",
    "                The answer should strictly be an exact extracted text from the contents.\n",
    "                If an exact extracted text does not answer the question, answer with a brief and precise answer derived from the contents.\n",
    "                Reply in the format:\n",
    "                \"answer\"\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"How big is the ANTISCAM dataset? \"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"220 human-human dialogs\"},          \n",
    "        { \"role\": \"user\", \"content\": f\"Contents of the research paper:{contents}\"},\n",
    "        { \"role\": \"user\", \"content\": f\"Question related to the paper:{question}\"}\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prompt_2(contents,question):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a question-answering system.\n",
    "                The user will provide you with the contents of a research paper.\n",
    "                The user will also provide a question to be answered.\n",
    "                Answer the question using the contents provided by the user.\n",
    "                Reply in the format:\n",
    "                \"answer\"\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"How big is the ANTISCAM dataset? \"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"220 human-human dialogs\"},         \n",
    "        { \"role\": \"user\", \"content\": f\"Contents of the research paper:{contents}\"},\n",
    "        { \"role\": \"user\", \"content\": f\"Question related to the paper:{question}\"}\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prompt_3(contents,question):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                The user will provide you with the contents of a research paper and a question.\n",
    "                Answer the question based on the contents in 1 sentence.\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"How big is the ANTISCAM dataset? \"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"220 human-human dialogs\"},           \n",
    "        { \"role\": \"user\", \"content\": f\"Contents of the research paper:{contents}\"},\n",
    "        { \"role\": \"user\", \"content\": f\"Question related to the paper:{question}\"}\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prompt_4(contents,question):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a child who has to answer a question with no knowledge of the domain.\n",
    "                The user will provide you with the contents of a research paper.\n",
    "                The user will also provide a question to be answered.\n",
    "                Strictly answer in one sentence or less. \n",
    "                The answer should strictly be an exact extracted text from the contents.\n",
    "                If an exact extracted text does not answer the question, answer like a child reading the contents.\n",
    "                Reply in the format:\n",
    "                \"answer\"\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"How big is the ANTISCAM dataset? \"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"220 human-human dialogs\"},          \n",
    "        { \"role\": \"user\", \"content\": f\"Contents of the research paper:{contents}\"},\n",
    "        { \"role\": \"user\", \"content\": f\"Question related to the paper:{question}\"}\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50\n",
      "220 human-human dialogs.\n",
      "We design a hierarchical intent annotation scheme that separates on-task and off-task information, where on-task intents are task-specific actions and off-task intents are general dialog acts that convey syntax information.\n",
      "TransferTransfo and hybrid model.\n",
      "Automatic Evaluation Metrics: Perplexity, Response-Intent Prediction (RIP), Response-Slot Prediction (RSP), Extended Response-Intent Prediction (ERIP), Extended Response-Slot Prediction (ERSP).\n",
      "Human Evaluation Metrics: Fluency, Coherence, Engagement, Dialog length, Task Success Score.\n",
      "1/50\n",
      "We illustrate these differences by building a simple entity linker, OpenTapioca, which only uses data from Wikidata, and show that it is competitive with other systems with access to larger data sources for some tasks.\n",
      "2/50\n",
      "Liu et. al (2015) and Yang et. al (2012)\n",
      "Detection Error Trade-off (DET) curves.\n",
      "No, the paper does not mention whether their methods are fully supervised or not.\n",
      "Yes, they built a dataset of rumors consisting of 202 confirmed rumors.\n",
      "Chinese\n",
      "The Cambridge dictionary defines a rumor as information of doubtful or unconfirmed truth.\n",
      "3/50\n",
      "LDA, Doc-NADE, HTMM, and GMNTM.\n",
      "generative model evaluation and document classification\n",
      "4/50\n",
      "CoNLL2003, OntoNotes 5.0, OntoNotes 4.0 (Chinese part), MSRA, Weibo NER, and Resume NER.\n",
      "By using relative positional encoding ($R_{t-j}$) in the attention score calculation, the attention mechanism in the Transformer can distinguish different directions and distances between tokens.\n",
      "In this paper, the proposed TENER model outperforms previous models in the six NER datasets, achieving state-of-the-art performance without considering pre-trained language models or designed features.\n",
      "5/50\n",
      "KALM achieves an accuracy of 95.6% and KALM-QA achieves an accuracy of 95% for parsing the queries.\n",
      "SEMAFOR, SLING, and Stanford KBP system.\n",
      "MetaQA dataset BIBREF14.\n",
      "6/50\n",
      "i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach; ii.) an investigation on the quality of existing Italian word embeddings for this task; iii.) a comparison against a state-of-the-art discrete classifier.\n",
      "FBK-HLT BIBREF23 is the baseline system used in this paper.\n",
      "7/50\n",
      "Candidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.\n",
      "First, they trained domain-specific word embeddings using the Word2Vec CBOW model and then used k-means clustering to cluster the embeddings of the gender-associated words.\n",
      "over 300K sentences\n",
      "The centroid of the cluster was used as a strong baseline label.\n",
      "8/50\n",
      "Data-driven models usually respond to abuse by ranking low in general and often producing responses that can be interpreted as flirtatious.\n",
      "9960 HITs from 472 crowd workers.\n",
      "14 response types\n",
      "9/50\n",
      "10/50\n",
      "Turkish, Finnish, Czech, German, Spanish, Catalan, and English.\n",
      "gold morphological features\n",
      "11/50\n",
      "The Semantic Scholar corpus, Springer Nature's SciGraph, The Textbook Question Answering corpus, Wikipedia, Flickr30K, and COCO.\n",
      "The captions in the paper are in English.\n",
      "The paper explores the ad-hoc approaches of HolE, Vecsigrafo, Embedding network, 2WayNet, VSE++, and DSVE-loc.\n",
      "The supervised baselines they compared with are the direct combination baseline and the supervised pre-training baseline.\n",
      "From the Semantic Scholar corpus and Springer Nature's SciGraph, they extracted figures and captions from scientific publications.\n",
      "12/50\n",
      "Weka baseline BIBREF5.\n",
      "\"Our intra-sentence attention RNN was able to outperform the Weka baseline on the development dataset, but performed worse than the baseline on the test dataset, indicating a lack of generalization.\"\n",
      "training, validation and test datasets provided for the shared task BIBREF5\n",
      "13/50\n",
      "Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it.\n",
      "English, French, and Arabic.\n",
      "(a) directness or indirectness of the text, (b) offensiveness, disrespectfulness, hatefulness, fear out of ignorance, abusiveness, or normalcy, (c) the attribute based on which it discriminates against an individual or a group of people, (d) the name of the discriminated group, and (e) the sentiment of the annotators towards the content.\n",
      "The final dataset is composed of a pilot corpus of 100 tweets per language, and comparable corpora of 5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets.\n",
      "14/50\n",
      "SWT may be used for semantic disambiguation in MT, support the translation of named entities, address the non-standard language problem, translate knowledge bases, and adapt the MT system to user characteristics.\n",
      "The challenges associated with the use of Semantic Web technologies in Machine Translation include the lack of large bilingual datasets for non-European languages, the poor run-time performance of reasoners, the disambiguation of named entities, the handling of non-standard speech and idioms, and the need to address reordering errors and lexical and syntactic ambiguity.\n",
      "The paper does not mention any other obstacles to automatic translations besides the ones mentioned in the abstract.\n",
      "15/50\n",
      "root mean square, zero crossing rate, moving window average, kurtosis, and power spectral entropy\n",
      "USC-TIMIT database (for data set A) and a dataset generated in the experiment (for data set B).\n",
      "16/50\n",
      "sentiment classification (SST-2), natural language inference (MNLI-m, MNLI-mm, QNLI) and pairwise semantic equivalence (QQP)\n",
      "17/50\n",
      "news articles and corresponding highlights from the DailyMail website.\n",
      "They use the DailyMail news highlights corpus.\n",
      "18/50\n",
      "This background knowledge is crucial for solving many difficult, ambiguous natural language problems in coreference resolution and question answering, as well as the creation of other reasoning machines.\n",
      "Intrinsic geometry of spaces of learned representations refers to the inherent structure and relationships within the embedding space that are automatically enforced during the learning process, resulting in globally consistent structured predictions of the ontology.\n",
      "19/50\n",
      "Yes, the paper mentions that they pre-trained their models on the Stanford Sentiment Treebank, which is a sentiment corpus.\n",
      "We extracted word unigrams and bigrams, and used term frequencies (TF) and Inverse document-frequency (IDF) to transform these features. We also included word embeddings pretrained on large corpora, sentiment features from manually constructed lexica, and pre-trained sentiment embeddings.\n",
      "English and Spanish.\n",
      "20/50\n",
      "AI2D-RST contains three graphs: Grouping, Connectivity, and Discourse structure.\n",
      "The annotation for AI2D was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk.\n",
      "Expert annotations and crowd-sourced annotations are compared in terms of their representations of diagrammatic structures and the performance of various graph neural network architectures in classifying nodes from both resources.\n",
      "Amazon Mechanical Turk\n",
      "Two expert annotators who have linguistic training.\n",
      "21/50\n",
      "HAN (Hierarchical Attention Network) and CNN (Convolutional Neural Network).\n",
      "The dataset they use is based on Wikipedia crawl data, specifically using the \"List of Controversial articles\" overview page of 2018 and 2009 as a seed set of controversial articles.\n",
      "lexical methods trained on common (e.g. fulltext) features\n",
      "22/50\n",
      "64M segments from YouTube videos.\n",
      "64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens.\n",
      "The features derived from the videos are 1500-dimensional vectors, extracted from the video frames at 1-second intervals.\n",
      "23/50\n",
      "\"an established NMT architecture based on LSTMs and implementing the attention mechanism.\"\n",
      "Symbolic rewriting is the process of transforming one symbolic expression into another using a set of predefined rules or equations.\n",
      "24/50\n",
      "anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors.\n",
      "20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set.\n",
      "25/50\n",
      "F-Score Driven Model II achieves a new state-of-the-art NER system in Chinese social media.\n",
      "Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media.\n",
      "Sina Weibo service in China.\n",
      "A modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media.\n",
      "26/50\n",
      "NLI (XNLI dataset), document classification (MLDoc dataset), intent classification, POS tagging, and NER.\n",
      "MMTE improves upon the bilingual baseline on low resource language pairs, but performs worse than the bilingual baseline on high resource language pairs.\n",
      "These languages were chosen based on the intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by their mNMT model.\n",
      "27/50\n",
      "We evaluate the dialogue using both automatic evaluation metrics such as perplexity and BLEU, as well as human evaluation where annotators compare dialogues based on engagingness, interestingness, and humanness.\n",
      "We observe a significant gap between the cross-lingual model and other models.\n",
      "We adapt the Poly-encoder model by using the Google Translate API to translate target languages query to English as the input to the model, then translate the English response back to the target language.\n",
      "Chinese, French, Indonesian, Italian, Korean, and Japanese.\n",
      "28/50\n",
      "We automatically create a test set from the OpenSubtitles corpus.\n",
      "Our baseline models include a standard bidirectional RNN model with attention, a concatenation model where each sentence is concatenated with the preceding sentence, a multi-encoder architecture with hierarchical attention, and a context-agnostic Transformer.\n",
      "s-hier, s-t-hier, s-hier-to-2, concat21, BIBREF8\n",
      "English and German.\n",
      "29/50\n",
      "In this paper, they use a simple entity linking algorithm that directly links the mention to the entity with the greatest commonness score, which is calculated based on the anchor links in Wikipedia.\n",
      "An MLP (Multi-Layer Perceptron) with three dense layers.\n",
      "FIGER (GOLD) BIBREF0 and BBN BIBREF5.\n",
      "30/50\n",
      "We recruited one domain expert to annotate the sentences in the dataset.\n",
      "The model's F1-score is used for evaluation.\n",
      "The model gives an F1-score of 0.89 for the concept recognition task.\n",
      "abb: represents abbreviations such as TRL representing Technology Readiness Level.\n",
      "grp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.\n",
      "syscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.\n",
      "opcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.\n",
      "seterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.\n",
      "event: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.\n",
      "org: represents an organization such as `NASA', `aerospace industry', etc.\n",
      "art: represents names of artifacts or instruments such as `AS1300'\n",
      "cardinal: represents numerical values such as `1', `100', 'one' etc.\n",
      "loc: represents location-like entities such as component facilities or centralized facility.\n",
      "mea: represents measures, features, or behaviors such as cost, risk, or feasibility.\n",
      "BERT BIBREF16\n",
      "3700 sentences.\n",
      "31/50\n",
      "precision, recall, and F-measure\n",
      "The hand-crafted features used are the position of the sentence, sentence length, and tense.\n",
      "The word embeddings used in the study are specific word embeddings trained on a specific corpus, inspired by Tang et al.'s Sentiment-Specific Word Embedding model.\n",
      "Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors, and specific word vectors.\n",
      "Argumentative zoning is a process of assigning rhetorical status to sentences in scientific articles, which helps provide readers with a general discourse context to better understand, compare, and analyze scientific ideas.\n",
      "32/50\n",
      "We used Twitter's REST API to collect the tweet objects of these tweets.\n",
      "Spam Detection 1: We use the Twitter spam detection method proposed by Wu et al. BIBREF4 . It uses the Word2Vec and Doc2Vec models to encode the tweets into a vector representation, which is fed to a MLP classifier in order to classify the tweets as spam or not-spam. We use the same methodology to classify tweets in our dataset as blackmarket or genuine. Spam Detection 2: For baseline 2, we consider the approach proposed by Rajdev et. al. BIBREF11 . They proposed flat and hierarchical classifications approaches with few of the standard set of features which can classify spam, fake and legitimate tweets. We use their experimental setup with Random Forest classifier on our dataset.\n",
      "English\n",
      "YouLikeHits and Like4Like.\n",
      "33/50\n",
      "English, French, and Chinese.\n",
      "6-layer decoder.\n",
      "10-layer encoder.\n",
      "CorefNqg, Mp-Gsn, and Xlm are the baselines used in the research paper.\n",
      "34/50\n",
      "Human judges prefer stories that were generated hierarchically by first creating a premise and creating a full story based on it with a seq2seq model.\n",
      "Perplexity and prompt ranking accuracy.\n",
      "Language Models, seq2seq models, Ensemble models, and KNN models.\n",
      "The convolutional language model from BIBREF4 is used to generate the premise.\n",
      "WritingPrompts forum.\n",
      "35/50\n",
      "word2vec, fastText, GloVe, Baroni, SL999.\n",
      "SICK, STSB, MRPC, SICK-R, and SICK-E.\n",
      "HCTI BIBREF5, HTCI, and InferSent BIBREF23.\n",
      "36/50\n",
      "Fleiss's Kappa for inter-annotator agreement.\n",
      "170 workers\n",
      "Neighborhood overlap, downward overlap, upward overlap, inward overlap, outward overlap, and bidirectional overlap.\n",
      "Aggressive language, Repetition, Harmful intent, Visibility among peers, Power imbalance.\n",
      "Aggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive. The user either addresses a group or individual, and the message contains at least one phrase that could be described as confrontational, derogatory, insulting, threatening, hostile, violent, hateful, or sexually abusive. Repetition: (rep) The target user has received at least two aggressive messages in total (either from the author or from another user in the visible thread). Harmful intent: (harm) The tweet was designed to tear down or disadvantage the target user by causing them distress or by harming their public image. The target does not respond agreeably as to a joke or an otherwise lighthearted comment. Visibility among peers: (peer) At least one other user besides the target has liked, retweeted, or responded to at least one of the author's messages. Power imbalance: (power) Power is derived from authority and perceived social advantage. Celebrities and public figures are more powerful than common users. Minorities and disadvantaged groups have less power. Bullies can also derive power from peer support.\n",
      "37/50\n",
      "We evaluated the similarity of the generated texts with training data objectively and the humor content subjectively.\n",
      "CrowdTruth and Subreddits.\n",
      "inspirational quotes\n",
      "38/50\n",
      "Multiple filters of the same shape.\n",
      "25.1% relative improvement in MRR and 1% absolute improvement in Hits@10.\n",
      "39/50\n",
      "BERT, BERT with Bi-LSTM, DenseNet, HighwayLSTM, BIMPM, Sim-Transformer\n",
      "CoNLL03 dataset, Yahoo Answer Classification Dataset, and \"Quora-Question-Pair\" dataset.\n",
      "40/50\n",
      "0.874 averaged weighted kappa value.\n",
      "Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication.\n",
      "933 manually identified adpositions.\n",
      "41/50\n",
      "Fake News Dataset\n",
      "SVM, Logistic Regression, ANN, LSTM, and Random Forest.\n",
      "42/50\n",
      "Ablation study to explore the importance of different feature families.\n",
      "achieving 98.2% in the test data\n",
      "gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.\n",
      "further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.\n",
      "43/50\n",
      "The sentence alignment quality is evaluated according to a 5-point scale used in KocabiyikogluETAL:18, which includes categories such as wrong alignment, partial alignment with slightly compositional translational equivalence, partial alignment with compositional translation and additional or missing information, correct alignment with compositional translation and few additional or missing information, and correct alignment and fully compositional translation.\n",
      "The evaluation of the audio-text alignment quality was conducted according to a 3-point scale: wrong alignment, partial alignment with some missing words or sentences, and correct alignment allowing non-spoken syllables at the start or end.\n",
      "44/50\n",
      "10.3% and 5.3%\n",
      "45/50\n",
      "Our method outperforms existing methods in terms of performance, but the exact improvement is not mentioned in the contents of the research paper.\n",
      "standard SICK (Sentences Involving Compositional Knowledge) dataset\n",
      "46/50\n",
      "SRE18 development and SRE18 evaluation datasets.\n",
      "Table 8. Subsystem performance on SRE18 DEV and EVAL set.\n",
      "47/50\n",
      "Yes, they reduce language variation of text by enhancing frequencies of semantically related words in the similarity sets.\n",
      "economic and political domains\n",
      "WordNet, EuroVoc, and RuThes.\n",
      "48/50\n",
      "\"Drawing upon these definitions, we define hate speech as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\"\n",
      "logistic regression with L2 regularization\n",
      "33,458 Twitter users.\n",
      "85.4 million tweets.\n",
      "49/50\n",
      "On the SST-2 sentiment classification dataset, not fine-tuning all of the layers leads to improved quality.\n",
      "No, they only test against the base variant of BERT and RoBERTa.\n"
     ]
    }
   ],
   "source": [
    "for i,datum in enumerate(data_reduced):\n",
    "    print('{}/{}'.format(i, len(data_reduced)))\n",
    "    # example_content = \"To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.\"\n",
    "    for qa in datum['qas']:\n",
    "        contents = qa['evidence']\n",
    "        LLM_answer = Prompt_4(contents,qa['question'])\n",
    "        print(LLM_answer)\n",
    "        qa['GPT_Answer'] = LLM_answer\n",
    "save_json(data_reduced, r'Data\\responses_P4_fs.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt used for openAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(datum_sentences):\n",
    "    sentence_list = [\" \".join(sentence_word_list) for sentence_word_list in datum_sentences] \n",
    "    paragraph = \" \".join(sentence_list)\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punctuation_with_whitespace(input_string):\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    return input_string.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = json.load(open(r\"Data\\responses_P4_fs.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryam\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryam\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\aryam\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/50\n",
      "3/50\n",
      "4/50\n",
      "5/50\n",
      "6/50\n",
      "7/50\n",
      "8/50\n",
      "9/50\n",
      "10/50\n",
      "11/50\n",
      "12/50\n",
      "13/50\n",
      "14/50\n",
      "15/50\n",
      "16/50\n",
      "17/50\n",
      "18/50\n",
      "19/50\n",
      "20/50\n",
      "21/50\n",
      "22/50\n",
      "23/50\n",
      "24/50\n",
      "25/50\n",
      "26/50\n",
      "27/50\n",
      "28/50\n",
      "29/50\n",
      "30/50\n",
      "31/50\n",
      "32/50\n",
      "33/50\n",
      "34/50\n",
      "35/50\n",
      "36/50\n",
      "37/50\n",
      "38/50\n",
      "39/50\n",
      "40/50\n",
      "41/50\n",
      "42/50\n",
      "43/50\n",
      "44/50\n",
      "45/50\n",
      "46/50\n",
      "47/50\n",
      "48/50\n",
      "49/50\n"
     ]
    }
   ],
   "source": [
    "scorer = Scorers()\n",
    "rel = st.relevance()\n",
    "fl = st.fluency()\n",
    "cr = st.Correctness()\n",
    "for i,datum in enumerate(data_final):\n",
    "    print('{}/{}'.format(i, len(data_final)))\n",
    "    for qa in datum['qas']:\n",
    "        bert_scores = []\n",
    "        ner_overlap = []\n",
    "        writer_summ = qa['answers']\n",
    "        llm_summ = qa['GPT_Answer']\n",
    "        readability = st.Readability(llm_summ)\n",
    "        formality = st.Formality(llm_summ)\n",
    "        bleu_1,bleu_2,bleu_3,bleu_4 = scorer.compute_bleu(writer_summ,llm_summ)\n",
    "        \n",
    "        qa['bleu_1'] = bleu_1\n",
    "        qa['bleu_2'] = bleu_2\n",
    "        qa['bleu_3'] = bleu_3\n",
    "        qa['bleu_4'] = bleu_4\n",
    "\n",
    "        rogue = scorer.compute_rouge(writer_summ,llm_summ)\n",
    "        qa['rogue'] = rogue\n",
    "\n",
    "        meteor = scorer.compute_meteor(writer_summ,llm_summ)\n",
    "        qa['meteor'] = meteor\n",
    "        qa['QA_Relevance'] = rel.default(qa['question'],qa['GPT_Answer'])\n",
    "        qa['CA_Relevance'] = rel.default(qa['evidence'],qa['GPT_Answer'])\n",
    "        qa['fluency'] = fl.default(qa['GPT_Answer'])\n",
    "        for ref in writer_summ:\n",
    "        #     # bert_scores.append(scorer.Bert_Score([ref],[llm_summ]))\n",
    "            ner_overlap.append(cr.default(ref,qa['GPT_Answer']))\n",
    "        qa['Correctness'] = max(ner_overlap)\n",
    "        # qa['bert_score'] = max(bert_scores)\n",
    "        qa['readability_LLM'] = readability.default()\n",
    "        qa['formality_LLM'] = formality.default()\n",
    "\n",
    "save_json(data_final, r'Data/scores_P4_fs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50\n",
      "1/50\n",
      "2/50\n",
      "3/50\n",
      "4/50\n",
      "5/50\n",
      "6/50\n",
      "7/50\n",
      "8/50\n",
      "9/50\n",
      "10/50\n",
      "11/50\n",
      "12/50\n",
      "13/50\n",
      "14/50\n",
      "15/50\n",
      "16/50\n",
      "17/50\n",
      "18/50\n",
      "19/50\n",
      "20/50\n",
      "21/50\n",
      "22/50\n",
      "23/50\n",
      "24/50\n",
      "25/50\n",
      "26/50\n",
      "27/50\n",
      "28/50\n",
      "29/50\n",
      "30/50\n",
      "31/50\n",
      "32/50\n",
      "33/50\n",
      "34/50\n",
      "35/50\n",
      "36/50\n",
      "37/50\n",
      "38/50\n",
      "39/50\n",
      "40/50\n",
      "41/50\n",
      "42/50\n",
      "43/50\n",
      "44/50\n",
      "45/50\n",
      "46/50\n",
      "47/50\n",
      "48/50\n",
      "49/50\n"
     ]
    }
   ],
   "source": [
    "temp = json.load(open(r\"Data\\scores_P4_fs.json\"))\n",
    "max_rogue_1_f = 0.0\n",
    "max_rogue_2_f = 0.0\n",
    "max_rogue_l_f = 0.0\n",
    "for i,datum in enumerate(temp):\n",
    "    print('{}/{}'.format(i, len(data_final)))\n",
    "    for qa in datum['qas']:\n",
    "        for rogue_list in qa[\"rogue\"]:\n",
    "            for rogue_dict in rogue_list:\n",
    "                max_rogue_1_f = max(max_rogue_1_f, rogue_dict[\"rouge-1\"][\"f\"])\n",
    "                max_rogue_2_f = max(max_rogue_2_f, rogue_dict[\"rouge-2\"][\"f\"])\n",
    "                max_rogue_l_f = max(max_rogue_l_f, rogue_dict[\"rouge-l\"][\"f\"])\n",
    "        qa['rogue'] = {\"rogue-1\":max_rogue_1_f,\"rogue-2\":max_rogue_2_f,\"rogue-l\":max_rogue_l_f}\n",
    "save_json(temp, r'Data/results/scores_P4_fs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50\n",
      "1/50\n",
      "2/50\n",
      "3/50\n",
      "4/50\n",
      "5/50\n",
      "6/50\n",
      "7/50\n",
      "8/50\n",
      "9/50\n",
      "10/50\n",
      "11/50\n",
      "12/50\n",
      "13/50\n",
      "14/50\n",
      "15/50\n",
      "16/50\n",
      "17/50\n",
      "18/50\n",
      "19/50\n",
      "20/50\n",
      "21/50\n",
      "22/50\n",
      "23/50\n",
      "24/50\n",
      "25/50\n",
      "26/50\n",
      "27/50\n",
      "28/50\n",
      "29/50\n",
      "30/50\n",
      "31/50\n",
      "32/50\n",
      "33/50\n",
      "34/50\n",
      "35/50\n",
      "36/50\n",
      "37/50\n",
      "38/50\n",
      "39/50\n",
      "40/50\n",
      "41/50\n",
      "42/50\n",
      "43/50\n",
      "44/50\n",
      "45/50\n",
      "46/50\n",
      "47/50\n",
      "48/50\n",
      "49/50\n",
      "[[' 3,044 sentences in 100 dialogs', '220 human-human dialogs', '220 human-human dialogs. ', '3,044 sentences in 100 dialogs', '  220 human-human dialogs.  3,044 sentences in 100 dialogs', '220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. ', '3,044 sentences in 100 dialogs'], ['using a role-playing task on the Amazon Mechanical Turk platform and collecting typed conversations', 'Separate on-task and off task intents and annotate on task for data set specific intents, while annotating  off task intents with a fixed set of general intents.', 'On-task dialog are annotated as on-task intents , the other dialog are annotated as pre-defined off-task intents.', 'separate on-task and off-task intents', 'on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task', 'off-task content is too general to design task-specific intents, we choose common dialog acts as the categories', '  separate on-task and off-task intents on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task off-task content is too general to design task-specific intents, we choose common dialog acts as the categories', 'we design a hierarchical intent annotation scheme for non-collaborative tasks. We first separate on-task and off-task intents. As on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific intents, we choose common dialog acts as the categories. ', 'In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme', 'For specific tasks, we also design a semantic slot annotation scheme for annotating sentences based on their semantic content. We identify 13 main semantic slots in the anti-scam task, for example, credit card numbers. We present a detailed semantic slot annotation in Table TABREF3. Following BIBREF1, we segment each conversation turn into single sentences and then annotate each sentence rather than turns.', '  we design a hierarchical intent annotation scheme for non-collaborative tasks. We first separate on-task and off-task intents. As on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific intents, we choose common dialog acts as the categories.  In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme For specific tasks, we also design a semantic slot annotation scheme for annotating sentences based on their semantic content. We identify 13 main semantic slots in the anti-scam task, for example, credit card numbers. We present a detailed semantic slot annotation in Table TABREF3. Following BIBREF1, we segment each conversation turn into single sentences and then annotate each sentence rather than turns.', 'using a hierarchical scheme where on-task intents uses task-related intents for representation and off-task intents chooses dialog acts that convey the syntax information'], ['TransferTransfo and Hybrid ', 'TransferTransfo', ' hybrid model', '  TransferTransfo  hybrid model', 'TransferTransfo', 'Hybrid', '  TransferTransfo Hybrid', 'TransferTransfo The vanilla TransferTransfo framework', 'Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA', '  TransferTransfo The vanilla TransferTransfo framework Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA'], ['Perplexity', 'Response-Intent Prediction (RIP)', 'Response-Slot Prediction (RSP)', 'Extended Response-Intent Prediction (ERIP) ', 'Extended Response-Slot Prediction (ERSP) ', 'Fluency', 'Coherence ', 'Engagement', 'Dialog length ', 'Task Success Score (TaskSuc)', '  Perplexity Response-Intent Prediction (RIP) Response-Slot Prediction (RSP) Extended Response-Intent Prediction (ERIP)  Extended Response-Slot Prediction (ERSP)  Fluency Coherence  Engagement Dialog length  Task Success Score (TaskSuc)', 'Perplexity ', 'Response-Intent Prediction (RIP)', 'Response-Slot Prediction (RSP)', 'Extended Response-Intent Prediction (ERIP)', 'Extended Response-Slot Prediction (ERSP)', 'Fluency ', 'Coherence ', 'Engagement ', 'Dialog length (Length) ', 'Task Success Score (TaskSuc)', '  Perplexity  Response-Intent Prediction (RIP) Response-Slot Prediction (RSP) Extended Response-Intent Prediction (ERIP) Extended Response-Slot Prediction (ERSP) Fluency  Coherence  Engagement  Dialog length (Length)  Task Success Score (TaskSuc)', \"Fluency Fluency is used to explore different models' language generation quality.\\n\\nCoherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.\\n\\nEngagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.\\n\\nDialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.\\n\\nTask Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score.\", 'Automatic evaluation metrics (Perplexity (PPl), Response-Intent Prediction (RIP), Response-Slot Prediction(RSP), Extended Response-Intent Prediction(ERIP),  Extended Response-Slot Prediction (ERSP)) and Human Evaluation Metrics (Fluency, Coherence, Engagement, Lenhth, TaskSuc)', 'Automatic metrics used: Perplexity, RIP, RSP, ERIP ERSP.\\nHuman evaluation metrics used: Fluency, Coherence, Engagement, Dialog length and Task Success Score.'], ['The model improves the state of the art performance for the ISTEX dataset (F1 micro: 0.870, F1 macro: 0.858) and for the Microposts 2016 dataset (F1 micro:  0.087).', 'The micro and macro f1-scores of this model are 0.482 and 0.399 on the AIDA-CoNLL dataset, 0.087 and 0.515 on the Microposts 2016 dataset, 0.870 and 0.858 on the ISTEX-1000 dataset, 0.335 and 0.310 on the RSS-500 dataset', 'The accuracy '], ['two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented.', 'Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter.', '  two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter.', 'Liu et. al (2015)', 'Yang et. al (2012)', '  Liu et. al (2015) Yang et. al (2012)', 'They compare against two other methods that apply message-,user-, topic- and propagation-based features and rely on an SVM classifier. One perform early rumor detection and operates with a delay of 24 hrs, while the other requires a cluster of 5 repeated messages to judge them for rumors.', 'Liu et. al (2015) ', 'Yang et. al (2012)', '  Liu et. al (2015)  Yang et. al (2012)', 'Liu et al. (2015) and Yang et al. (2012)'], ['accuracy to evaluate effectiveness', 'Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability', 'throughput per second', '  accuracy to evaluate effectiveness Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability throughput per second', 'The metrics are accuracy, detection error trade-off curves and computing efficiency', 'accuracy ', 'Detection Error Trade-off (DET) curves', 'efficiency of computing the proposed features, measured by the throughput per second', '  accuracy  Detection Error Trade-off (DET) curves efficiency of computing the proposed features, measured by the throughput per second', 'Accuracy compared to two state-of-the-art baselines'], ['No. They additionally use similarity to previously detected rumors to make the decision of whether a document is likely to be a rumor'], ['Yes, consisting of trusted resources, rumours and non-rumours'], ['Chinese', 'Mandarin Chinese', 'Chinese', 'Mandarin Chinese (see table 3)'], ['the presence of information unconfirmed by the official media is construed as an indication of being a rumour. ', 'information of doubtful or unconfirmed truth', 'information that is not fact- and background-checked and thoroughly investigated for authenticity', 'Information of doubtful or unconfirmed truth'], ['LDA', 'Doc-NADE', 'HTMM', 'GMNTM', '  LDA Doc-NADE HTMM GMNTM', 'LDA BIBREF2', 'Doc-NADE BIBREF24', 'HTMM BIBREF9', 'GMNTM BIBREF12', '  LDA BIBREF2 Doc-NADE BIBREF24 HTMM BIBREF9 GMNTM BIBREF12', 'LDA BIBREF2 ', 'Doc-NADE BIBREF24', 'HTMM BIBREF9', 'GMNTM BIBREF12', 'LDA BIBREF2 ', 'Doc-NADE BIBREF24', 'HTMM BIBREF9 ', 'GMNTM BIBREF12', '  LDA BIBREF2  Doc-NADE BIBREF24 HTMM BIBREF9 GMNTM BIBREF12 LDA BIBREF2  Doc-NADE BIBREF24 HTMM BIBREF9  GMNTM BIBREF12'], ['generative model evaluation (i.e. test set perplexity) and document classification', 'generative model evaluation', 'document classification', '  generative model evaluation document classification', 'generative model evaluation (i.e. test set perplexity)', 'document classification', '  generative model evaluation (i.e. test set perplexity) document classification', 'generative document evaluation task', 'document classification task', 'topic2sentence task', '  generative document evaluation task document classification task topic2sentence task'], ['CoNLL2003', 'OntoNotes 5.0', 'OntoNotes 4.0.', 'Chinese NER dataset MSRA', 'Weibo NER', 'Resume NER', '  CoNLL2003 OntoNotes 5.0 OntoNotes 4.0. Chinese NER dataset MSRA Weibo NER Resume NER', 'CoNLL2003 ', 'OntoNotes 5.0', 'OntoNotes 4.0', 'MSRA ', 'Weibo', 'Resume ', '  CoNLL2003  OntoNotes 5.0 OntoNotes 4.0 MSRA  Weibo Resume ', 'CoNLL2003', 'OntoNotes 5.0', 'OntoNotes 4.0', 'MSRA', 'Weibo NER', 'Resume NER', '  CoNLL2003 OntoNotes 5.0 OntoNotes 4.0 MSRA Weibo NER Resume NER', 'CoNLL2003', 'OntoNotes 5.0', 'BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part', 'Chinese NER dataset MSRA', 'Weibo NER', 'Resume NER', '  CoNLL2003 OntoNotes 5.0 BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part Chinese NER dataset MSRA Weibo NER Resume NER'], ['by using an relative sinusodial positional embedding and unscaled attention', 'calculate the attention scores  which can  distinguish different directions and distances', 'Self-attention mechanism is changed to allow for direction-aware calculations'], ['we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features'], ['95.6% on knowledge authoring, 95% on the manually constructed QA dataset and 100% accuracy on the MetaQA dataset', 'KALM achieves an accuracy of 95.6%', 'KALM-QA achieves 100% accuracy', '  KALM achieves an accuracy of 95.6% KALM-QA achieves 100% accuracy', 'KALM-QA achieves an accuracy of 95% for parsing the queries', 'The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy', '  KALM-QA achieves an accuracy of 95% for parsing the queries The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy', 'KALM achieves an accuracy of 95.6%, KALM-QA achieves 95% accuracy on the manually constructured general questions dataset based on the 50 logical frames and achieves 100% accuracy on MetaQA dataset'], ['SEMAFOR', 'SLING', 'Stanford KBP ', '  SEMAFOR SLING Stanford KBP ', 'SEMAFOR', 'SLING', 'Stanford KBP system', '  SEMAFOR SLING Stanford KBP system', 'SEMAFOR, SLING, and Stanford KBP system', 'BIBREF14', '  SEMAFOR, SLING, and Stanford KBP system BIBREF14'], ['dataset consisting 250 sentences adapted from FrameNet exemplar sentences, dataset consisting general questions based on 50 logical framesderived from FrameNet, MetaQA dataset', 'first dataset is manually constructed general questions based on the 50 logical frames', 'second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions', '  first dataset is manually constructed general questions based on the 50 logical frames second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions', 'a manually created dataset of 50 logical frames mostly derived from FrameNet, a manually constructed general questions dataset based on the 50 logical frames and MetaQA dataset', ' manually constructed general questions based on the 50 logical frames', 'MetaQA dataset', '   manually constructed general questions based on the 50 logical frames MetaQA dataset'], ['adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach', 'investigation on the quality of existing Italian word embeddings for this task', 'a comparison against a state-of-the-art discrete classifier', '  adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach investigation on the quality of existing Italian word embeddings for this task a comparison against a state-of-the-art discrete classifier', '(1) Using seq2seq for event detection and classification in Italian (2) Investigating quality of Italian word embeddings for this task (3) Comparison to state-of-the-art discrete classifier', 'the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach', 'an investigation on the quality of existing Italian word embeddings for this task', 'a comparison against a state-of-the-art discrete classifier', 'pre-trained models and scripts running the system', '  the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach an investigation on the quality of existing Italian word embeddings for this task a comparison against a state-of-the-art discrete classifier pre-trained models and scripts running the system', 'Adapting a seq2seq neural system to event detection and classification for Italian, investigating the quality of existing embeddings for the task, and comparing against a state-of-the-art discrete classifier.'], [' cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features', 'FBK-HLT - a cascade of two SVM classifiers (one for detection and one for classification)', 'FBK-HLT BIBREF23'], ['Given a cluster, our algorithm proceeds with the following three steps:\\n\\nSense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.\\n\\nCandidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.\\n\\nCandidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.\\n\\nIn steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well.', 'Candidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.', 'They automatically  label the cluster using WordNet and context-sensitive strengths of domain-specific word embeddings', \"Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering\"], ['First, we trained domain-specific word embeddings using the Word2Vec BIBREF33 CBOW model ($w \\\\in R^{100}$). Then, we used k-means clustering to cluster the embeddings of the gender-associated words. Since k-means may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors.', 'First, we trained domain-specific word embeddings', 'Then, we used k-means clustering to cluster the embeddings of the gender-associated words', '  First, we trained domain-specific word embeddings Then, we used k-means clustering to cluster the embeddings of the gender-associated words', 'First, they  trained domain-specific word embeddings using the Word2Vec  model, then used k-means clustering to cluster the embeddings of the gender-associated words.', 'The authors first generated a set of words which are associated with each gender, then built domain-specific word embeddings and used k-means clustering to cluster the gendered word associations together. '], ['300K sentences in each dataset', 'each consisting of over 300K sentences', 'Celeb dataset: 15917 texts and 342645 sentences\\nProfessor dataset: 283973 texts and 976677 sentences', 'Celebrity Dataset has 15,917 texts, 342,645 sentences, and the Female Male Proportions are  0.67/ 0.33. \\nProfessor Dataset has 283,973 texts, 976, 667 sentences, and the Femal Male Proportions are 0.28./ 0,72'], ['The authors contrasted human evaluations against a random baseline, and used the centroid of the cluster as a strong baseline.', 'the top 4 predicted labels and the centroid of the cluster', 'the top 4 predicted labels and the centroid of the cluster as a strong baseline label'], ['either by refusing politely, or, with flirtatious responses, or, by retaliating', 'Data-driven systems rank low in general', 'politely refuse', 'politely refuses', 'flirtatious responses', '  politely refuse politely refuses flirtatious responses', 'flirt; retaliation'], ['600K', '9960', '9960 HITs from 472 crowd workers', '9960 HITs'], ['14', '12', '14'], ['agglutinative and fusional languages', 'agglutinative and fusional', 'Turkish, Finnish, Czech, German, Spanish, Catalan and English'], ['char3 slides a character window of width $n=3$ over the token', 'lemma of the token', 'additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units.', 'characters', 'character sequences', '  char3 slides a character window of width $n=3$ over the token lemma of the token additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. characters character sequences', 'For all languages, morph outputs the lemma of the token followed by language specific morphological tags', 'additional information for some languages, such as parts-of-speech tags for Turkish', '  For all languages, morph outputs the lemma of the token followed by language specific morphological tags additional information for some languages, such as parts-of-speech tags for Turkish', 'language specific morphological tags', 'morph outputs the lemma of the token followed by language specific morphological tags', 'semantic roles of verbal predicates', '  morph outputs the lemma of the token followed by language specific morphological tags semantic roles of verbal predicates'], ['The Semantic Scholar corpus ', \"Springer Nature's SciGraph\", 'The Textbook Question Answering corpus', 'Wikipedia', 'Flickr30K and COCO', \"  The Semantic Scholar corpus  Springer Nature's SciGraph The Textbook Question Answering corpus Wikipedia Flickr30K and COCO\", 'The Semantic Scholar corpus', \"Springer Nature's SciGraph\", 'The Textbook Question Answering corpus', 'January 2018 English Wikipedia dataset', 'Flickr30K', 'COCO', \"  The Semantic Scholar corpus Springer Nature's SciGraph The Textbook Question Answering corpus January 2018 English Wikipedia dataset Flickr30K COCO\", 'The Semantic Scholar corpus', \"Springer Nature's SciGraph\", 'The Textbook Question Answering corpus', 'Wikipedia', 'Flickr30K', 'COCO', \"  The Semantic Scholar corpus Springer Nature's SciGraph The Textbook Question Answering corpus Wikipedia Flickr30K COCO\", 'Semantic Scholar corpus BIBREF21 (SemScholar)', \"Springer Nature's SciGraph\", 'Textbook Question Answering corpus BIBREF23', 'Wikipedia', 'Flickr30K', 'COCO', \"  Semantic Scholar corpus BIBREF21 (SemScholar) Springer Nature's SciGraph Textbook Question Answering corpus BIBREF23 Wikipedia Flickr30K COCO\"], ['English', 'English'], ['HolE', 'Vecsigrafo', '  HolE Vecsigrafo', 'Embedding network', '2WayNet', 'VSE++', 'DSVE-loc)', '  Embedding network 2WayNet VSE++ DSVE-loc)'], ['direct combination', 'supervised pre-training', '  direct combination supervised pre-training', 'direct combination baseline', 'supervised pre-training baseline', '  direct combination baseline supervised pre-training baseline', 'The direct combination baseline ', 'The supervised pre-training baseline', '  The direct combination baseline  The supervised pre-training baseline'], ['The Semantic Scholar corpus', \"Springer Nature's SciGraph\", \"  The Semantic Scholar corpus Springer Nature's SciGraph\", 'scientific publications', 'middle school science curricula', '  scientific publications middle school science curricula', 'scientific literature', 'SN SciGraph and AI2 Semantic Scholar'], ['Weka baseline BIBREF5', 'Weka', ' Weka baseline BIBREF5'], ['Pearson correlation on sadness test data is 0.52, on joy test data is .537, on anger test data is 0.47, on fear data is 0.561.', '0.689 on development and 0.522 on test set', 'For the anger dataset, our experiments showed that GloVe embeddings of dimension 50 outperformed others, obtaining an average gain of 0.066 correlation over embeddings of size 25 and of 0.021 for embeddings of size 100.', 'In the joy dataset, our experiments showed us that GloVe vectors of dimension 50 again outperformed others, in this case obtaining an average correlation gain of 0.052 ( INLINEFORM0 ) over embeddings of size 100, and of 0.062 ( INLINEFORM1 ) for size 25.', 'On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 ( INLINEFORM0 ) and 0.11 ( INLINEFORM1 ) for sizes 25 and 100, respectively.', 'on the sadness datasets again we experimentally observed that using embeddings of 50 offered the best results, with a statistically significant average gain of 0.092 correlation points INLINEFORM0 over size 25', '  For the anger dataset, our experiments showed that GloVe embeddings of dimension 50 outperformed others, obtaining an average gain of 0.066 correlation over embeddings of size 25 and of 0.021 for embeddings of size 100. In the joy dataset, our experiments showed us that GloVe vectors of dimension 50 again outperformed others, in this case obtaining an average correlation gain of 0.052 ( INLINEFORM0 ) over embeddings of size 100, and of 0.062 ( INLINEFORM1 ) for size 25. On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 ( INLINEFORM0 ) and 0.11 ( INLINEFORM1 ) for sizes 25 and 100, respectively. on the sadness datasets again we experimentally observed that using embeddings of 50 offered the best results, with a statistically significant average gain of 0.092 correlation points INLINEFORM0 over size 25'], [' training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger', 'datasets provided for the shared task BIBREF5', 'Dataset of tweets provided for the shared task.', 'Dataset from shared task BIBREF5'], ['rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech', 'Hate speech is a text that contains one or more of the following aspects: directness, offensiveness, targeting a group or individual based on specific attributes, overall negativity.', ' in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis.'], ['English', 'French', 'Arabic', '  English French Arabic', 'English, French, and Arabic '], [' (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments', 'whether the text is direct or indirect', 'if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal', 'the attribute based on which it discriminates against an individual or a group of people', 'the name of this group', ' how the annotators feel about its content within a range of negative to neutral sentiments', '  whether the text is direct or indirect if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal the attribute based on which it discriminates against an individual or a group of people the name of this group  how the annotators feel about its content within a range of negative to neutral sentiments', '(a) whether the text is direct or indirect', '(b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal', '(c) the attribute based on which it discriminates against an individual or a group of people', '(d) the name of this group', '(e) how the annotators feel about its content within a range of negative to neutral sentiments', '  (a) whether the text is direct or indirect (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal (c) the attribute based on which it discriminates against an individual or a group of people (d) the name of this group (e) how the annotators feel about its content within a range of negative to neutral sentiments', 'Directness', 'Hostility', 'Target group', 'Target', 'Sentiment of the annotator', '  Directness Hostility Target group Target Sentiment of the annotator'], ['13 000 tweets', '13014', '5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets'], ['disambiguation', 'Named Entities', 'Non-standard speech', 'Translating KBs', '  disambiguation Named Entities Non-standard speech Translating KBs', 'disambiguation', 'NERD', ' non-standard language', 'translating KBs', '  disambiguation NERD  non-standard language translating KBs', 'Disambiguation', 'Named Entities', 'Non-standard speech', 'Translating KBs', '  Disambiguation Named Entities Non-standard speech Translating KBs', 'SWT can be applied to support the semantic disambiguation in MT: to  recognize ambiguous words before translation and  as a post-editing technique applied to  the output language. SWT may be used for translating KBs.'], ['syntactic disambiguation problem which as yet lacks good solutions', 'directly related to the ambiguity problem and therefore has to be resolved in that wider context', 'In rare cases SMT can solve this problem, but considering that new idiomatic expressions appear every day and most of them are isolated sentences, this challenge still remains open', '  syntactic disambiguation problem which as yet lacks good solutions directly related to the ambiguity problem and therefore has to be resolved in that wider context In rare cases SMT can solve this problem, but considering that new idiomatic expressions appear every day and most of them are isolated sentences, this challenge still remains open', 'reordering errors', ' lexical and syntactic ambiguity', '  reordering errors  lexical and syntactic ambiguity', 'SWT are hard to implement'], ['Excessive focus on English and European languages', 'limitations of SMT approaches for translating across domains', 'no-standard speech texts from users', 'morphologically rich languages', 'parallel data for training differs widely from real user speech', '  Excessive focus on English and European languages limitations of SMT approaches for translating across domains no-standard speech texts from users morphologically rich languages parallel data for training differs widely from real user speech', 'reordering errors'], ['We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0', ' So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel', 'We extracted Mel-frequency cepstrum (MFCC) as features for speech signal. We first extracted MFCC 13 features and then computed first and second order differentials (delta and delta-delta) thus having total MFCC 39 features. ', '  We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0  So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel We extracted Mel-frequency cepstrum (MFCC) as features for speech signal. We first extracted MFCC 13 features and then computed first and second order differentials (delta and delta-delta) thus having total MFCC 39 features. ', 'root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy', 'root mean square', 'zero crossing rate', 'moving window average', 'kurtosis', 'power spectral entropy', '  root mean square zero crossing rate moving window average kurtosis power spectral entropy', 'root mean square', 'zero crossing rate', 'moving window average', 'kurtosis', 'power spectral entropy', 'extracted 31(channels) X 5 or 155 features', '  root mean square zero crossing rate moving window average kurtosis power spectral entropy extracted 31(channels) X 5 or 155 features'], [' two types of simultaneous speech EEG recording databases ', 'The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment.', 'Speech EEG recording collected from male and female subjects under different background noises', 'For database A five female and five male subjects took part in the experiment.', 'For database B five male and three female subjects took part in the experiment.', '  For database A five female and five male subjects took part in the experiment. For database B five male and three female subjects took part in the experiment.'], ['MNLI-m, MNLI-mm, SST-2, QQP, QNLI', 'LadaBERT -1, -2 achieves state of art on all datasets namely, MNLI-m MNLI-mm, SST-2, QQP, and QNLI. \\nLadaBERT-3 achieves SOTA on the first four dataset. \\nLadaBERT-4 achieves SOTA on MNLI-m, MNLI-mm, and QNLI ', 'SST-2', 'MNLI-m', 'MNLI-mm', 'QNLI', 'QQP', '  SST-2 MNLI-m MNLI-mm QNLI QQP', 'LadaBERT-1 and LadaBERT-2  on MNLI-m, MNLI-mm, SST-2, QQP and QNLI .\\nLadaBERT-3  on MNLI-m, MNLI-mm, SST-2, and QQP . LadaBERT-4  on MNLI-m, MNLI-mm and QNLI .'], ['news articles', 'news'], ['DUC 2002 document summarization corpus', 'our own DailyMail news highlights corpus', '  DUC 2002 document summarization corpus our own DailyMail news highlights corpus', 'DUC 2002', 'our own Dailymail news highlights corpus', '  DUC 2002 our own Dailymail news highlights corpus', 'the benchmark DUC 2002 document summarization corpus', 'DailyMail news highlights corpus', '  the benchmark DUC 2002 document summarization corpus DailyMail news highlights corpus', 'DailyMail news articles'], ['hypernym relations', 'the collection of information that an ordinary person would have', 'Hypernymy or is-a relations between words or phrases', 'Knowledge than an ordinary person would have such as transitive entailment relation, complex ordering, compositionality, multi-word entities'], ['In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.', 'The intrinsic geometry is that the general concept embedding should be smaller than the specific concept embedding in every coordinate of the embeddings', 'the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than local relation predictions'], [\"No, they used someone else's pretrained model. \"], ['unigrams and bigrams', 'word2vec', 'manually constructed lexica', 'sentiment embeddings', '  unigrams and bigrams word2vec manually constructed lexica sentiment embeddings'], ['2', '2', '2 (Spanish and English)'], ['spatial organisation ', 'discourse structure', '  spatial organisation  discourse structure', 'node types that represent different diagram elements', 'The same features are used for both AI2D and AI2D-RST for nodes with layout information', 'discourse relations', 'information about semantic relations', '  node types that represent different diagram elements The same features are used for both AI2D and AI2D-RST for nodes with layout information discourse relations information about semantic relations', 'grouping, connectivity, and discourse structure '], ['The annotation for AI2D was\\ncreated by crowd-sourced non-expert annotators on AMT while AI2D-RST covers a subset of diagrams from AI2D annotated by trained experts'], ['by using them as features in classifying diagrams and\\ntheir parts using various graph neural networks.', 'Expert annotators incorporate domain knowledge from multimodality theory while non-expert cannot but they are less time-consuming and use less resources.', 'results are not entirely comparable due to different node types', 'more reasonable to compare architectures', '  results are not entirely comparable due to different node types more reasonable to compare architectures'], ['Amazon Mechanical Turk'], ['Annotators trained on multimodality theory', 'domain knowledge from multimodality theory', 'Those who have domain knowledge on multimodal communication and annotation.'], ['Recurrent Neural Networks', 'Convolutional Neural Networks', '  Recurrent Neural Networks Convolutional Neural Networks', 'RNNs and CNNs', 'HAN BIBREF10', 'CNN BIBREF11', '  HAN BIBREF10 CNN BIBREF11', 'CNN', 'RNN', '  CNN RNN'], ['Clueweb09', 'Clueweb09 derived dataset', 'new dataset based on Wikipedia crawl data', '  Clueweb09 derived dataset new dataset based on Wikipedia crawl data', 'the Clueweb09 derived dataset ', 'dataset based on Wikipedia crawl data', '  the Clueweb09 derived dataset  dataset based on Wikipedia crawl data', 'Clueweb09 derived dataset', 'Wikipedia crawl data', '  Clueweb09 derived dataset Wikipedia crawl data'], ['semantic representations of word embeddings'], ['64M segments from YouTube videos', 'YouCook2 ', 'sth-sth', '  YouCook2  sth-sth', 'About 64M segments from YouTube videos comprising a total of 1.2B tokens.'], ['64M video segments with 1.2B tokens', '64M', '64M segments from YouTube videos', 'INLINEFORM0 B tokens', 'vocabulary of 66K wordpieces', '  64M segments from YouTube videos INLINEFORM0 B tokens vocabulary of 66K wordpieces'], ['1500-dimensional vectors similar to those used for large scale image classification tasks.', 'features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks', '1500-dimensional vectors, extracted from the video frames at 1-second intervals'], ['NMT architecture BIBREF10', 'architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism', 'LSTM with attention'], ['It is a process of translating a set of formal symbolic data to another set of formal symbolic data.', 'Symbolic rewriting is the method to rewrite ground and nonground data from one to another form using rules.'], ['The experts define anchors and the model learns correlations between the anchors and latent topics.', 'anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors', 'They use an anchored information theoretic topic modeling using Correlation Explanation and  information bottleneck.'], ['20 Newsgroups', 'i2b2 2008 Obesity Challenge BIBREF22 data set', '  20 Newsgroups i2b2 2008 Obesity Challenge BIBREF22 data set', '20 Newsgroups ', 'i2b2 2008 Obesity Challenge', '  20 Newsgroups  i2b2 2008 Obesity Challenge', ' i2b2 2008 Obesity Challenge BIBREF22', '20 Newsgroups', '   i2b2 2008 Obesity Challenge BIBREF22 20 Newsgroups'], ['For Named Entity, F-Score Driven I model had 49.40 F1 score,  and F-Score Driven II model had 50.60  F1 score. In case of Nominal Mention, the scores were 58.16 and 59.32', '50.60 on Named Entity and 59.32 on Nominal Mention', 'Best proposed model achieves F1 score of 50.60, 59.32, 54.82, 20.96 on Named Entity, Nominam Mention, Overall, Out of vocabulary respectively.', 'Best F1 score obtained is 54.82% overall'], ['Peng and Dredze peng-dredze:2016:P16-2'], ['Sina Weibo service', 'Sina Weibo'], ['Peng and Dredze peng-dredze:2016:P16-2', 'Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service', '  Peng and Dredze peng-dredze:2016:P16-2 Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service', 'Peng and Dredze peng-dredze:2016:P16-2', 'a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2'], ['These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER.', 'NLI (XNLI dataset)', 'document classification (MLDoc dataset)', 'intent classification', 'POS tagging', 'NER', '  NLI (XNLI dataset) document classification (MLDoc dataset) intent classification POS tagging NER', 'NLI (XNLI dataset)', 'document classification (MLDoc dataset)', ' intent classification', 'sequence tagging tasks: POS tagging', 'NER', '  NLI (XNLI dataset) document classification (MLDoc dataset)  intent classification sequence tagging tasks: POS tagging NER', 'NLI', 'document classification', 'intent classification', 'POS tagging', 'NER', '  NLI document classification intent classification POS tagging NER'], ['we see that the gains are more pronounced in low resource languages'], ['These languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model', 'For a given language pair, $l$, let $D_l$ be the size of the available parallel corpus. Then if we adopt a naive strategy and sample from the union of the datasets, the probability of the sample being from language pair $l$ will be $p_l=\\\\frac{D_l}{\\\\Sigma _lD_l}$. However, this strategy would starve low resource language pairs. To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs). We set $T=5$ for a balanced sampling strategy. To control the contribution of each language pair when constructing the vocabulary, we use the same temperature based sampling strategy with $T=5$. Our SPM vocabulary has a character coverage of $0.999995$.', 'intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model'], ['They use automatic evaluation using perplexity and BLEU scores with reference to the human-annotated responses and human evaluation on interestingness, engagingness, and humanness.', 'perplexity (ppl.) and BLEU', 'which of the two dialogues is better in terms of engagingness, interestingness, and humanness', '  perplexity (ppl.) and BLEU which of the two dialogues is better in terms of engagingness, interestingness, and humanness', 'perplexity', 'BLEU', 'ACUTE-EVA', '  perplexity BLEU ACUTE-EVA'], ['significant gap between the cross-lingual model and other models', 'Table TABREF20', '  significant gap between the cross-lingual model and other models Table TABREF20', 'BLUE score is lower by 4 times than that of the best multilingual model.'], ['Translate source sentence to English with Google Translate API and then translate the result to the target language with Poly-encoder.', 'M-Bert2Bert', 'M-CausalBert', 'Bert2Bert', 'CausalBert', 'Poly-encoder BIBREF75', 'XNLG', '  M-Bert2Bert M-CausalBert Bert2Bert CausalBert Poly-encoder BIBREF75 XNLG', 'Google Translate API'], ['Chinese', 'French', 'Indonesian', 'Italian', 'Korean', 'Japanese', '  Chinese French Indonesian Italian Korean Japanese', 'English', 'Chinese', 'French', 'Indonesian', 'Italian', 'Korean', 'Japanese', '  English Chinese French Indonesian Italian Korean Japanese', 'Chinese, French, Indonesian, Italian, Korean, and Japanese'], ['It is automatically created from the OpenSubtitles corpus.'], ['bidirectional RNN model with attention', 'concat22', 's-hier', 's-t-hier', 's-hier-to-2', 'Transformer-base', 'concat22', 'concat21', '  bidirectional RNN model with attention concat22 s-hier s-t-hier s-hier-to-2 Transformer-base concat22 concat21', ' standard bidirectional RNN model with attention', 'A standard context-agnostic Transformer', '   standard bidirectional RNN model with attention A standard context-agnostic Transformer', 'standard bidirectional RNN model with attention', 'concat22', 's-hier A multi-encoder architecture with hierarchical attention', 's-t-hier ', 's-hier-to-2 ', 'A standard context-agnostic Transformer.', 'concat22', 'concat21', 'BIBREF8', '  standard bidirectional RNN model with attention concat22 s-hier A multi-encoder architecture with hierarchical attention s-t-hier  s-hier-to-2  A standard context-agnostic Transformer. concat22 concat21 BIBREF8'], ['standard bidirectional RNN model with attention', 'concat22', 's-hier', 's-t-hier', 's-hier-to-2', 'standard context-agnostic Transformer', 'concat22', 'concat21', 'BIBREF8', '  standard bidirectional RNN model with attention concat22 s-hier s-t-hier s-hier-to-2 standard context-agnostic Transformer concat22 concat21 BIBREF8', 'bidirectional RNN', 'concat22', 's-hier', 's-t-hier', 's-hier-to-2', 'Transformer-base', 'concat22', 'concat21', 'BIBREF8', '  bidirectional RNN concat22 s-hier s-t-hier s-hier-to-2 Transformer-base concat22 concat21 BIBREF8', 'a standard bidirectional RNN model with attention', 'concat22 ', 's-hier', 's-t-hier', 's-hier-to-2', 'concat21 ', 'BIBREF8 ', '  a standard bidirectional RNN model with attention concat22  s-hier s-t-hier s-hier-to-2 concat21  BIBREF8 '], ['English', 'German', '  English German', 'English', 'German ', '  English German ', 'English ', 'German ', '  English  German '], ['They use an EL algorithm that links the mention to the entity with the help of the greatest commonness score.', 'The mention is linked to the entity with the greatest commonness score.', 'we use a simple EL algorithm that directly links the mention to the entity with the greatest commonness score. Commonness BIBREF17, BIBREF18 is calculated base on the anchor links in Wikipedia. It estimates the probability of an entity given only the mention string.'], ['BiLSTMs ', 'MLP ', '  BiLSTMs  MLP ', 'BiLSTM with a three-layer perceptron', 'BiLSTM'], ['FIGER (GOLD) BIBREF0', 'BBN BIBREF5', '  FIGER (GOLD) BIBREF0 BBN BIBREF5', 'FIGER (GOLD) ', 'BBN', '  FIGER (GOLD)  BBN', 'FIGER (GOLD)', 'BBN', '  FIGER (GOLD) BBN'], ['1', 'One domain expert.'], ['F1-score', 'precision, recall, f1-score, and support', 'Precision, recall, f1-score, and support.'], ['F1-score of $0.89$', 'The model gives an F1-score of $0.89$ for the concept recognition task.', ' F1-score of $0.89$'], ['Based on operation and system concepts, the labels are abb, grp, syscon, opcon, seterm, event, org, art, cardinal, loc and mea.', \"BIO Labelling Scheme\\nabb: represents abbreviations such as TRL representing Technology Readiness Level.\\n\\ngrp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.\\n\\nsyscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.\\n\\nopcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.\\n\\nseterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.\\n\\nevent: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.\\n\\norg: represents an organization such as `NASA', `aerospace industry', etc.\\n\\nart: represents names of artifacts or instruments such as `AS1300'\\n\\ncardinal: represents numerical values such as `1', `100', 'one' etc.\\n\\nloc: represents location-like entities such as component facilities or centralized facility.\\n\\nmea: represents measures, features, or behaviors such as cost, risk, or feasibility.\", '1. abb\\n2. grp\\n3. syscon\\n4. opcon\\n5. seterm\\n6. event\\n7. org\\n8. art\\n9. cardinal\\n10. loc\\n11. mea'], ['BERT', 'BERT '], ['3700 sentences', '3700 sentences ', 'roughly 3700 sentences at the word-token level'], ['Precision, recall and F-measure.', 'precision', 'recall', 'F-measure', '  precision recall F-measure', 'precision, recall and F-measure'], ['position of sentence', 'sentence length', 'tense', 'qualifying adjectives', 'meta-discourse features', '  position of sentence sentence length tense qualifying adjectives meta-discourse features', ' sentences with their rhetorical status '], ['INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 )', 'Sentiment-Specific Word Embedding', 'word2vec', '  Sentiment-Specific Word Embedding word2vec', 'word2vec', 'Sentiment-Specific Word Embedding', '  word2vec Sentiment-Specific Word Embedding'], ['sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors', 'Averaging the vectors of the words in a sentence, directly learning paragraph vectors using PV-DM, taking average of the SSWE of the words in a sentence.', ' average the vectors in word sequence', 'training paragraph vectors', 'Sentiment-Specific Word Embedding', '   average the vectors in word sequence training paragraph vectors Sentiment-Specific Word Embedding'], [' Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences', 'process of assigning rhetorical status to the extracted sentences', 'a process of assigning rhetorical status to the extracted sentences'], ['crawled two blackmarket sites', \"used Twitter's REST API\", \"  crawled two blackmarket sites used Twitter's REST API\", \"By crawling YouLikeHits and Like4Like sites and then using Twitter's REST API\", \"We used Twitter's REST API\"], [' spam detection method proposed by Wu et al. BIBREF4 ,  spam detection method  proposed by Rajdev et. al. BIBREF11,   feature vector by concatenating the tweet content features with Random Forest,  feature vector by concatenating the tweet content features with Multi-layer Perception and  feature vector by concatenating the tweet content features with Support Vector Machine.', 'Wu et al. BIBREF4', 'Rajdev et. al. BIBREF11', '  Wu et al. BIBREF4 Rajdev et. al. BIBREF11', 'Word2Vec and Doc2Vec to encode the tweets, then MLP classifier; Random Forest classifier on a standard set of features'], ['English'], ['Credit-based Freemium services', 'YouLikeHits and Like4Like'], ['English', 'French', 'Chinese', '  English French Chinese', 'English', 'Chinese', 'French', '  English Chinese French', 'English/French/Chinese'], ['pre-trained Xnlg', '6-layer decoder', '  pre-trained Xnlg 6-layer decoder', '6 transformer layers, each layer containing 1024 hidden units, 8 attention heads, and GELU activations.', 'denoising auto-encoding (DAE) objective BIBREF24'], ['pre-trained Xnlg with a 10-layer encoder', 'denoising auto-encoding (DAE) objective BIBREF24', '10 transformer layers, each layer containing 1024 hidden units, 8 attentions heads, and GELU activations.'], ['CorefNqg BIBREF33', 'Mp-Gsn BIBREF31', 'Xlm BIBREF5', 'Xlm Fine-tuning', 'Pipeline (Xlm)', 'Pipeline (Xlm) with Google Translator', '  CorefNqg BIBREF33 Mp-Gsn BIBREF31 Xlm BIBREF5 Xlm Fine-tuning Pipeline (Xlm) Pipeline (Xlm) with Google Translator', 'CorefNqg', 'Mp-Gsn', 'Xlm', 'Pipeline (Xlm)', 'Pipeline (Xlm) with Google Translator', '  CorefNqg Mp-Gsn Xlm Pipeline (Xlm) Pipeline (Xlm) with Google Translator', 'CorefNqg BIBREF33 ', 'Mp-Gsn BIBREF31', 'Xlm BIBREF5', '  CorefNqg BIBREF33  Mp-Gsn BIBREF31 Xlm BIBREF5'], ['human preference', 'triple pairing task', 'hierarchical generation', '  triple pairing task hierarchical generation', 'Accuracy at pairing stories with the prompts used to generate them; accuracy of prompt ranking'], ['perplexity', 'prompt ranking accuracy', '  perplexity prompt ranking accuracy', 'model perplexity on the test set ', 'prompt ranking accuracy', '  model perplexity on the test set  prompt ranking accuracy', 'perplexity ', 'prompt ranking accuracy', '  perplexity  prompt ranking accuracy'], ['gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism', 'LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention', 'an ensemble of two Conv seq2seq with self-attention models', 'KNN model', '  gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention an ensemble of two Conv seq2seq with self-attention models KNN model', 'Language Models', 'seq2seq', 'Ensemble', 'KNN', '  Language Models seq2seq Ensemble KNN', 'Language Models', 'seq2seq: using LSTMs and convolutional seq2seq architectures', 'Conv seq2seq with decoder self-attention', 'an ensemble of two Conv seq2seq with self-attention models', 'KNN model', '  Language Models seq2seq: using LSTMs and convolutional seq2seq architectures Conv seq2seq with decoder self-attention an ensemble of two Conv seq2seq with self-attention models KNN model'], ['convolutional language model from BIBREF4', ' convolutional language model from BIBREF4', 'convolutional language model'], ['online forum', \"Reddit's WritingPrompts forum\"], ['word2vec ', 'fastText ', 'GloVe ', 'Baroni ', 'SL999 ', '  word2vec  fastText  GloVe  Baroni  SL999 ', 'word2vec', 'fastText', 'GloVe', 'Baroni', 'SL999', '  word2vec fastText GloVe Baroni SL999'], ['STSB ', 'SICK', 'MRPC', '  STSB  SICK MRPC', 'STSB, SICK, MRPC', 'SICK', 'STSB', 'MRPC', '  SICK STSB MRPC'], ['ECNU', 'HCTI', '  ECNU HCTI', 'HCTI BIBREF5', 'InferSent BIBREF23 ', '  HCTI BIBREF5 InferSent BIBREF23 ', 'ECNU BIBREF6', 'HCTI BIBREF5', '  ECNU BIBREF6 HCTI BIBREF5'], [\"Fleiss's Kappa\", \"Fleiss's Kappa \"], ['170', 'three '], ['Relative positions of the author and target accounts in the directed following network by\\ncomputing modified versions of Jaccard’s similarity index, friends count, followers count, verified status, number of tweets posted within 6 months.', 'Downward overlap, upward overlap, inward overlap, outward overlap, bidirectional overlap, count of friends of each user, count of followers of each user, users verified status, number of tweets posted within six-month snapshots', 'Neighborhood Overlap', ' count the friends and followers of each user, their verified status, and the number of tweets posted within six-month snapshots of their timelines', '  Neighborhood Overlap  count the friends and followers of each user, their verified status, and the number of tweets posted within six-month snapshots of their timelines'], ['Aggressive language', 'Repetition', 'Harmful intent', 'Visibility among peers', 'Power imbalance', '  Aggressive language Repetition Harmful intent Visibility among peers Power imbalance'], ['They define cyberbullying as aggressive language, repetition, harmful intent, visibility among peers, and power imbalance', 'cyberbullying is a complex social phenomenon, different from the simpler notion of cyberaggression', 'A public display of intention to “inflict injury or discomfort” upon a weaker victim through repeated acts of aggression.'], ['similarity of the generated texts with training data objectively', 'humor content subjectively', 'syntactic correctness of the generated sentences', '  similarity of the generated texts with training data objectively humor content subjectively syntactic correctness of the generated sentences', 'For measuring the similarity of the generated texts we used Phrase Overlap match and K-gram-Jaccard similarity as our criteria', 'To evaluate the quality of the generated jokes, quotes, or tweets we rely on human judgment', '  For measuring the similarity of the generated texts we used Phrase Overlap match and K-gram-Jaccard similarity as our criteria To evaluate the quality of the generated jokes, quotes, or tweets we rely on human judgment', 'Phrase Overlap match and K-gram-Jaccard similarity'], ['CrowdTruth and Subreddits', 'CrowdTruth ', 'Subreddits', '  CrowdTruth  Subreddits', 'CrowdTruth', 'Subreddits', '  CrowdTruth Subreddits'], ['inspirational'], ['1x3 filter size is used in convolutional layers.', '1x3'], [' improvements of INLINEFORM0 in MRR (which is about 25.1% relative improvement)', 'INLINEFORM1 % absolute improvement in Hits@10', '   improvements of INLINEFORM0 in MRR (which is about 25.1% relative improvement) INLINEFORM1 % absolute improvement in Hits@10', '0.105 in MRR and 6.1 percent points in Hits@10 on FB15k-237', 'On FB15k-237 dataset it outperforms 0.105 in MRR and  6.1% absolute improvement in Hits@10'], ['BERT', 'BERT adding a Bi-LSTM on top', 'DenseNet BIBREF33 and HighwayLSTM BIBREF34', 'BERT+ BIMPM', 'remove the first bi-LSTM of BIMPM', 'Sim-Transformer', '  BERT BERT adding a Bi-LSTM on top DenseNet BIBREF33 and HighwayLSTM BIBREF34 BERT+ BIMPM remove the first bi-LSTM of BIMPM Sim-Transformer', 'BERT, BERT+ Bi-LSTM ,  BERT+ DenseNet, BERT+HighwayLSTM,   Ensembled model, BERT+ BIMPM, BERT+ BIMPM(first bi-LSTM removed),  BERT + Sim-Transformer .', 'BERT, BERT + Bi-LSTM, BERT + HighwayLSTM, BERT + DenseNet, Ensembled Model, BERT + BIMPM, BERT + Sim-Transformer'], ['CoNLL03 ', 'Yahoo Answer Classification Dataset', '“Quora-Question-Pair” dataset 1', '  CoNLL03  Yahoo Answer Classification Dataset “Quora-Question-Pair” dataset 1', 'CoNLL03', ' Yahoo Answer Classification Dataset', '“Quora-Question-Pair” dataset 1', 'CoNLL03 dataset BIBREF5', 'Yahoo Answer Classification Dataset', ' “Quora-Question-Pair” dataset', '  CoNLL03 dataset BIBREF5 Yahoo Answer Classification Dataset  “Quora-Question-Pair” dataset'], [' two inter-annotator agreement ', \"aw agreement and Cohen's kappa across three annotators computed by averaging three pairwise comparisons\", \"   two inter-annotator agreement  aw agreement and Cohen's kappa across three annotators computed by averaging three pairwise comparisons\", 'Raw agreement is around .90 for this dataset.', 'The average agreement on scene, function and construal is 0.915'], ['The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.', 'Tokenization', 'Adposition Targets', 'Data Format', 'Reliability of Annotation', '  Tokenization Adposition Targets Data Format Reliability of Annotation', 'The corpus is jointly annotated by three native Mandarin Chinese speakers', 'Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication', 'Annotation was conducted in two phases', '  The corpus is jointly annotated by three native Mandarin Chinese speakers Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication Annotation was conducted in two phases'], ['933 manually identified adpositions', '20287'], ['https://github.com/Sairamvinay/Fake-News-Dataset\\n\\n'], ['SVM, Logistic Regression, ANN, LSTM, and Random Forest', 'Artificial Neural Network (ANN)', 'Long Short Term Memory networks (LSTMs)', ' Random Forest', 'Logistic Regression', ' Support Vector Machine (SVM)', '  Artificial Neural Network (ANN) Long Short Term Memory networks (LSTMs)  Random Forest Logistic Regression  Support Vector Machine (SVM)', 'SVM', 'Logistic Regression', 'ANN', 'LSTM', 'Random Forest', 'TFIDF', 'CV', 'W2V', '  SVM Logistic Regression ANN LSTM Random Forest TFIDF CV W2V'], ['Following groups of features are extracted:\\n- Numerical Features\\n- Language Models\\n- Clusters\\n- Latent Dirichlet Allocation\\n- Part-Of-Speech\\n- Bag-of-words', 'Numerical features, language models features, clusters, latent Dirichlet allocation, Part-of-Speech tags, Bag-of-words.', 'Numerical features, Language Models, Clusters, Latent Dirichlet Allocation, Part-Of-Speech tags, Bag-of-words'], ['Accuracy metric', 'accuracy', 'Accuracy'], ['gradient boosted trees', 'Light Gradient Boosting Machine'], ['the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not', 'Investigate the effectiveness of LDA to capture the subject of the essay.', 'investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used'], ['Through human evaluation on a 5-point scale for text alignment and 3-point scale for audio-text', 'The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18:\\n\\nWrong alignment\\n\\nPartial alignment with slightly compositional translational equivalence\\n\\nPartial alignment with compositional translation and additional or missing information\\n\\nCorrect alignment with compositional translation and few additional or missing information\\n\\nCorrect alignment and fully compositional translation\\n\\nThe evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\\n\\nWrong alignment\\n\\nPartial alignment, some words or sentences may be missing\\n\\nCorrect alignment, allowing non-spoken syllables at start or end.\\n\\nThe evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.', '5-point scale used in KocabiyikogluETAL:18'], ['Through a 3-point scale by annotators.', 'Wrong alignment', 'Partial alignment, some words or sentences may be missing', 'Correct alignment, allowing non-spoken syllables at start or end.', '  Wrong alignment Partial alignment, some words or sentences may be missing Correct alignment, allowing non-spoken syllables at start or end.', 'The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\\n\\nWrong alignment\\n\\nPartial alignment, some words or sentences may be missing\\n\\nCorrect alignment, allowing non-spoken syllables at start or end.\\n\\nThe evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.'], ['Outperforms PCNN+HATT by 10.3% and PCNN+BAG-ATT by 5.3%', '5.3 percent points', 'Compared to previous state-of-the-art approaches (i.e., PCNN+HATT and PCNN+BAG-ATT), the proposed model can also outperform them by a large margin, i.e., 10.3% and 5.3%'], ['In Semantic Relatedness task their model outperforms existing methods by more than 0.023 Pearson Correlation. In Textual Entailment task their model scores 0.004 accuracy lesser than MaxEnt', 'Their best implementation for semantic relatedness task comparison outperforms standard MaxEnt by 0,052 Pearson  Correlation.\\nTheir best implementation for Textual Entailment task comparison (84,2 accuracy) DOES NOT outperform standard SVM (84,6 accuracy).\\n', 'Best proposed result had 0.851 and  0.842 compared to best previous result of 0.828 and 0.846 on person correlation and accuracy respectively.'], ['SICK (Sentences Involving Compositional Knowledge) dataset ', 'SICK (Sentences Involving Compositional Knowledge) dataset'], ['SRE18 development and SRE18 evaluation datasets', 'SRE19', 'SRE04/05/06/08/10/MIXER6\\nLDC98S75/LDC99S79/LDC2002S06/LDC2001S13/LDC2004S07\\nVoxceleb 1/2\\nFisher + Switchboard I\\nCallhome+Callfriend'], ['primary system is the linear fusion of all the above six subsystems', 'eftdnn ', 'eftdnn'], ['Variation decreases when frequencies of synonyms is enhanced; variation increases when frequencies of synonyms, hyponyms, hypernyms are enhanced'], ['economic', 'political', '  economic political', ' news articles related to Islam and articles discussing Islam basics'], ['WordNet', 'European Union EuroVoc', 'RuThes', '  WordNet European Union EuroVoc RuThes', 'WordNet', 'EuroVoc', ' RuThes', '  WordNet EuroVoc  RuThes', 'WordNet ', 'EuroVoc ', 'RuThes ', '  WordNet  EuroVoc  RuThes '], ['language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group'], ['logistic regression', 'naïve Bayes', 'decision trees', 'random forests', 'linear SVMs', '  logistic regression naïve Bayes decision trees random forests linear SVMs', 'logistic regression', 'naïve Bayes', 'decision trees', 'random forests', 'linear SVM', '  logistic regression naïve Bayes decision trees random forests linear SVM', 'logistic regression, naïve Bayes, decision trees, random forests, and linear SVMs'], ['33,458', '33,458 Twitter users are orginally used, but than random sample of tweets is extracted resulting in smaller number or users in final dataset.', '33458'], ['85400000', '24,802 ', '24,802 labeled tweets'], ['SST-2'], ['For GLUE bencmark no, for dataset MRPC, SST-B, SST-2 and COLA yes.']]\n",
      "[['We collected 220 human-human dialogs for the ANTISCAM dataset.'], ['In the intent annotation scheme, on-task intents are defined based on key actions specific to each task, while off-task intents are categorized into common dialog acts that convey syntax information.'], ['TransferTransfo and hybrid model.'], ['The evaluation metrics used to evaluate the model performance are perplexity, response-intent prediction (RIP), response-slot prediction (RSP), extended response-intent prediction (ERIP), extended response-slot prediction (ERSP), fluency, coherence, engagement, dialog length, and task success score.'], ['We illustrate these differences by building a simple entity linker, OpenTapioca, which only uses data from Wikidata, and show that it is competitive with other systems with access to larger data sources for some tasks.'], ['Liu et. al (2015) and Yang et. al (2012).'], ['We report accuracy to evaluate effectiveness, as is usual in the literature.'], ['No, the methods mentioned in the paper are not fully supervised.'], ['Yes, they built their own dataset of rumors.'], ['answer'], ['The Cambridge dictionary defines a rumor as information of doubtful or unconfirmed truth.'], ['LDA, Doc-NADE, HTMM, and GMNTM.'], ['generative model evaluation and document classification'], ['CoNLL2003, OntoNotes 5.0, OntoNotes 4.0 (Chinese part), MSRA, Weibo NER, and Resume NER.'], ['By using specific learnable parameters and relative positional encoding, the attention score can distinguish different directions and distances between tokens.'], ['In this paper, we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features.'], ['KALM achieves an accuracy of 95.6%—much higher than the other systems.'], ['KALM, SEMAFOR, SLING, and Stanford KBP system.'], ['The second dataset they evaluated on is the MetaQA dataset BIBREF14, which contains almost 29,000 test questions and over 260,000 training questions.'], ['The contributions of this paper are: i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach; ii.) an investigation on the quality of existing Italian word embeddings for this task; iii.) a comparison against a state-of-the-art discrete classifier.'], ['The paper does not mention the baselines used.'], ['Our algorithm assigns the semantic concept label of a particular cluster by minimizing the total distance between the synsets of the chosen words in the cluster.'], ['They discover coherent word clusters by using k-means clustering on the embeddings of the gender-associated words.'], ['Over 300K sentences.'], ['The centroid of the cluster is used as a strong baseline label.'], ['Data-driven systems rank low in general.'], ['9960 HITs from 472 crowd workers.'], ['14 response types were evaluated.'], ['Turkish, Finnish, Czech, German, Spanish, Catalan and English.'], ['gold morphological features'], [\"The Semantic Scholar corpus, PDFFigures2, Springer Nature's SciGraph, Textbook Question Answering corpus, Wikipedia, Flickr30K, and COCO.\"], ['The captions in the paper are in English.'], ['The paper explores the ad-hoc approaches of HolE, Vecsigrafo, Embedding network, 2WayNet, VSE++, and DSVE-loc.'], ['The supervised baselines they compared with are the direct combination baseline and the supervised pre-training baseline.'], ['We extracted the figures and captions from the scientific literature, specifically from the Semantic Scholar corpus and the SN SciGraph corpus.'], ['The Weka baseline BIBREF5 was used as the baseline in the experiments.'], ['our intra-sentence attention RNN was able to outperform the Weka baseline on the development dataset, but performed worse than the baseline on the test dataset.'], ['The training, validation, and test datasets provided for the shared task BIBREF5 were used in the experiment.'], ['The paper does not explicitly provide a definition of hate speech.'], ['English, French, and Arabic.'], ['(a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments.'], ['The final dataset is composed of a pilot corpus of 100 tweets per language, and comparable corpora of 5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets.'], ['SWT have the potential to support semantic disambiguation in Machine Translation by recognizing ambiguous words in the source text and applying pre-editing and post-editing techniques, resolving the issue of common words used as proper nouns in the target language, addressing the problem of non-standard speech and idioms, and facilitating the translation of knowledge bases by utilizing semantic structures and bilingual embeddings.'], ['The challenges associated with the use of Semantic Web technologies in Machine Translation include the reliance on manually crafted rules in rule-based machine translation systems, the need for semantic disambiguation of polysemous and homonymous words, and the tedious implementation of pre-editing and post-editing techniques for translating common words.'], ['The other obstacles to automatic translations that are not mentioned in the abstract include the limitations of SMT approaches for translating across domains, the challenge of translating data from social networks that contain non-standard speech texts, the difficulty of translating among morphologically rich languages, and the discrepancies between parallel data for training and real user speech in the speech translation task.'], ['root mean square, zero crossing rate, moving window average, kurtosis, and power spectral entropy'], ['data set A and data set B'], ['LadaBERT achieves state-of-the-art on sentiment classification (SST-2), natural language inference (MNLI-m, MNLI-mm, QNLI), and pairwise semantic equivalence (QQP) datasets.'], ['news articles'], ['The researchers use two large-scale datasets, one for sentence extraction and another one for word extraction, which are created from hundreds of thousands of news articles and their corresponding highlights from the DailyMail website.'], ['The paper discusses the collection of information that an ordinary person would have, known as commonsense knowledge, which includes knowledge about relationships between words, hierarchical relations between multi-word phrases, and ontologies of hierarchical data.'], ['Intrinsic geometry of spaces of learned representations refers to the inherent structure and relationships within the embedding space that are automatically enforced during the process of learning representations.'], ['Yes, these models were pre-trained on larger corpora.'], ['We obtained a representation for each segment by averaging the embedding of each word in the segment.'], ['The dataset includes two languages: English and Spanish.'], ['AI2D-RST contains three graphs: Grouping, Connectivity, and Discourse structure.'], ['The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk.'], ['Expert annotations are compared to crowd-sourced annotations in terms of their representations of diagrammatic structures and whether the higher cost of expert annotations is justified.'], ['Amazon Mechanical Turk.'], ['Expert annotations, in the context of this study, are annotations made by individuals who possess domain knowledge from multimodality theory, as mentioned in the paper.'], ['RNNs and CNNs.'], ['The Clueweb09 derived dataset of BIBREF0 and a new dataset based on Wikipedia crawl data.'], ['lexical methods trained on common (e.g. fulltext) features'], ['Our training data consist of about 64M segments from YouTube videos.'], ['Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens.'], ['The features derived from the videos are 1500-dimensional vectors extracted from the video frames at 1-second intervals.'], ['We explored an NMT architecture based on LSTMs and implementing the attention mechanism.'], ['Symbolic rewriting refers to the process of transforming one symbolic expression or equation into another using a set of predefined rules or transformations.'], ['In this approach, they manually specify anchors, which are words that have high mutual information with one or more latent factors, to incorporate expert knowledge into their topic model.'], ['20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set.'], ['F-Score obtained is the result of the F-Score Driven Model II, which achieves a new state-of-the-art NER system in Chinese social media.'], ['Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media.'], ['Sina Weibo.'], ['A modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 and the same unlabelled text from Sina Weibo service in China.'], ['NLI, document classification, intent classification, POS tagging, and NER.'], ['MMTE improves upon the bilingual baseline on low resource language pairs, but performs worse than the bilingual baseline on high resource language pairs.'], ['These languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model.'], ['We evaluate dialogue using both automatic and human evaluation.'], ['We observe a significant gap between the cross-lingual model and other models.'], ['We adapt the Poly-encoder BIBREF75 model by using the Google Translate API to translate target languages (e.g., Chinese) query to English as the input to the model, then translate the English response back to the target language.'], ['Chinese, French, Indonesian, Italian, Korean, and Japanese.'], ['Yes, they automatically created their own test set from the OpenSubtitles corpus.'], ['The baselines mentioned in the paper are: a standard bidirectional RNN model with attention, a model that concatenates each sentence with one preceding sentence, a multi-encoder architecture with hierarchical attention, a model that considers two additional contexts (previous source sentence and previous target sentence), a model with an additional encoder for source context, a standard context-agnostic Transformer, a simple concatenation model, and a model trained on data where the preceding sentence is concatenated to the current one only on the source side.'], ['s-hier, s-t-hier, s-hier-to-2, concat21, BIBREF8'], ['English and German.'], ['In this paper, they use a simple entity linking algorithm that directly links the mention to the entity with the greatest commonness score, which is calculated based on the anchor links in Wikipedia.'], ['The model architecture they use is an MLP (Multi-Layer Perceptron) with three dense layers.'], ['FIGER (GOLD) BIBREF0 and BBN BIBREF5.'], ['We annotate roughly 3700 sentences at the word-token level using domain keywords and a domain expert.'], ['The model gives an F1-score of $0.89$ for the concept recognition task.'], ['The model gives an F1-score of $0.89$ for the concept recognition task.'], [\"abb: represents abbreviations such as TRL representing Technology Readiness Level.\\ngrp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.\\nsyscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.\\nopcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.\\nseterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.\\nevent: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.\\norg: represents an organization such as `NASA', `aerospace industry', etc.\\nart: represents names of artifacts or instruments such as `AS1300'\\ncardinal: represents numerical values such as `1', `100', 'one' etc.\\nloc: represents location-like entities such as component facilities or centralized facility.\\nmea: represents measures, features, or behaviors such as cost, risk, or feasibility.\"], ['BERT BIBREF16.'], ['The unique tag count is shown in Table 1.'], ['precision, recall and F-measure'], ['The hand-crafted features used are the position of the sentence, sentence length, and tense.'], [\"The word embeddings used in this study are specific word embeddings trained using the Sentiment-Specific Word Embedding model inspired by Tang et al.'s model.\"], ['The sentence embeddings are generated by averaging the vectors of the words in one sentence.'], ['Argumentative zoning is a process of assigning rhetorical status to sentences in scientific articles, which helps provide readers with a general discourse context to better link, compare, and analyze scientific ideas.'], ['We obtained the tweets by creating dummy accounts on blackmarket sites and recording the Tweet IDs of the tweets that were posted for gaining retweets.'], ['Spam Detection 1: We use the Twitter spam detection method proposed by Wu et al. BIBREF4.\\nSpam Detection 2: For baseline 2, we consider the approach proposed by Rajdev et. al. BIBREF11.'], ['English.'], ['YouLikeHits and Like4Like.'], ['In the first pre-training stage, they use a 15-language pre-trained XLM to initialize the parameters of their encoder and decoder.'], ['We use a 6-layer decoder for the architecture of the decoder.'], ['We use a 10-layer encoder with 1024 hidden units, 8 attention heads, and GELU activations.'], ['CorefNqg, Mp-Gsn, Xlm Fine-tuning, Pipeline (Xlm), and Pipeline (Xlm) with Google Translator.'], ['Human judges prefer stories that were generated hierarchically by first creating a premise and creating a full story based on it with a seq2seq model.'], ['Perplexity and prompt ranking accuracy.'], ['Language Models, seq2seq models, Ensemble models, and KNN models.'], ['The convolutional language model from BIBREF4 is used to generate the premise.'], [\"The stories are collected from Reddit's WritingPrompts forum.\"], ['word2vec, fastText, GloVe, Baroni, SL999.'], ['SICK, STSB, MRPC, SICK-R, and SICK-E.'], ['The state-of-the-art methods they compare with are SL999 embedding, HCTI, and InferSent.'], [\"Fleiss's Kappa.\"], ['170 workers.'], ['We measure the relative positions of the author and target accounts in the directed following network and use basic user account metrics such as friends, followers, verified status, and number of tweets posted within six-month snapshots of their timelines.'], ['Aggressive language, Repetition, Harmful intent, Visibility among peers, Power imbalance.'], ['The five criteria for defining cyberbullying are aggressive language, repetition, harmful intent, visibility among peers, and power imbalance.'], ['We evaluated the similarity of the generated texts with training data objectively and the humor content subjectively.'], ['The two sources for jokes are CrowdTruth and Subreddits.'], ['The system is trying to generate inspirational quotes.'], ['The size filters used in the convolution layer are of the same shape.'], ['25.1% relative improvement in MRR and INLINEFORM1 % absolute improvement in Hits@10.'], ['\"We implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top.\"'], ['CoNLL03 dataset, Yahoo Answer Classification Dataset, and \"Quora-Question-Pair\" dataset were used for testing.'], ['Agreement levels on scene role, function, and full construal are high for both phases, attesting to the validity of the annotation framework in Chinese.'], ['The corpus was annotated by three native Mandarin Chinese speakers, who received advanced training in theoretical and computational linguistics, and the annotation process involved cooperative and independent labeling of adposition targets followed by cooperative adjudication.'], ['Our corpus contains 933 manually identified adpositions.'], ['answer'], ['\"SVM, Logistic Regression, ANN, LSTM, and Random Forest.\"'], ['The paper explores the importance of different feature families in the essays.'], ['The system achieved 98.2% accuracy in the test data.'], ['gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft'], ['further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.'], ['The sentence alignment quality is evaluated according to a 5-point scale, which includes wrong alignment, partial alignment with slightly compositional translational equivalence, partial alignment with compositional translation and additional or missing information, correct alignment with compositional translation and few additional or missing information, and correct alignment and fully compositional translation.'], ['The evaluation of the audio-text alignment quality was conducted according to a 3-point scale: Wrong alignment, Partial alignment, and Correct alignment.'], ['10.3% and 5.3%'], ['Our method outperforms existing methods by achieving better performance compared to traditional machine learning methods such as SVM and MaxEnt approach.'], ['We use the standard SICK (Sentences Involving Compositional Knowledge) dataset for evaluation, which consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment.'], ['SRE18 development and SRE18 evaluation datasets.'], ['Table 8. Subsystem performance on SRE18 DEV and EVAL set.'], ['Yes, they reduce language variation of text by enhancing frequencies.'], ['The EuroVoc thesaurus explores the economic and political domains.'], ['WordNet, EuroVoc, and RuThes.'], ['\"Drawing upon these definitions, we define hate speech as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\"'], ['They train a logistic regression model with L2 regularization.'], ['33,458 Twitter users.'], ['85.4 million tweets.'], ['On SST-2, a sentiment classification dataset, not fine-tuning all of the layers leads to improved quality.'], ['On the smaller CoLA, SST-2, MRPC, and STS-B datasets, we comprehensively evaluate both models.']]\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(r\"Data\\responses_P4.json\"))\n",
    "references=[]\n",
    "candidates = []\n",
    "for i,datum in enumerate(data):\n",
    "    print('{}/{}'.format(i, len(data_final)))\n",
    "\n",
    "    for qa in datum['qas']:\n",
    "        references.append(qa[\"answers\"])\n",
    "        candidates.append([qa[\"GPT_Answer\"]])\n",
    "print(references)\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "BLEUscore = nltk.translate.bleu_score.corpus_bleu(references, candidates)\n",
    "BLEUscore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
