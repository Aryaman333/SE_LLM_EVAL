{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aryam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import jsonlines\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import openai\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "import itertools\n",
    "import string\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "import gensim.downloader as api\n",
    "import sys\n",
    "sys.path.insert(0, './Metrics')\n",
    "from Automatic_Metrics import Scorers\n",
    "from Linguistic_Metrics import LinguisticFeatures\n",
    "import Stylistic_Metrics as st\n",
    "import  Metrics.Stylistic_Metrics\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key =  '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(r\"Data\\qasper-test-v0.3.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "for paper_id, paper_info in data.items():\n",
    "    doc = {}\n",
    "    doc['doc_id'] = paper_id\n",
    "    doc['title'] = paper_info['title']\n",
    "    doc['qas'] = []\n",
    "    for qa in paper_info['qas']:\n",
    "        answers = []\n",
    "        evidence = \" \"\n",
    "\n",
    "        for ans in qa['answers']:\n",
    "            ext_spans_combined = \" \"\n",
    "            extractive_spans = ans['answer']['extractive_spans']\n",
    "            if len(extractive_spans)>1:\n",
    "                for es in extractive_spans:\n",
    "                    ext_spans_combined =ext_spans_combined+\" \"+es\n",
    "            free_form_answer = ans['answer']['free_form_answer']\n",
    "            ev = ans['answer']['evidence']\n",
    "            for e in ev:\n",
    "                if e not in evidence:\n",
    "                    evidence=evidence+\" \"+e\n",
    "            if extractive_spans and extractive_spans not in answers:\n",
    "                answers.append(extractive_spans)\n",
    "            if free_form_answer and not extractive_spans and free_form_answer not in answers :\n",
    "                answers.append(free_form_answer)\n",
    "            if ext_spans_combined != \" \" and ext_spans_combined not in answers :\n",
    "                answers.append(ext_spans_combined)\n",
    "        if answers:\n",
    "            qa_copy = qa.copy()\n",
    "            qa_copy['answers'] = answers\n",
    "            qa_copy['evidence'] = evidence\n",
    "            doc['qas'].append(qa_copy)\n",
    "    clean_data.append(doc)\n",
    "save_json(clean_data, r'Data\\data_cleaned.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = json.load(open(r\"Data\\data_cleaned.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(lst):\n",
    "    flattened = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            flattened.extend(flatten_list(item))\n",
    "        else:\n",
    "            flattened.append(item)\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,datum in enumerate(clean_data):\n",
    "    for qa in datum['qas']:\n",
    "        flattened_list = flatten_list(qa['answers'])\n",
    "        qa['answers'] = flattened_list\n",
    "save_json(clean_data, r'Data\\data_preprocessed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = json.load(open(r\"Data\\data_preprocessed.json\"))\n",
    "data_reduced = clean_data[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    completions = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature = 0.0,\n",
    "        messages=messages)\n",
    "    gpt_response = completions['choices'][0]['message']['content'].strip() \n",
    "    return gpt_response   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prompt_1(contents,question):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a NLP domain expert question answering system.\n",
    "                The user will provide you with the contents of a research paper.\n",
    "                The user will also provide a question to be answered.\n",
    "                Strictly answer in one sentence or less. \n",
    "                The answer should strictly be an exact extracted text from the contents.\n",
    "                If an exact extracted text does not answer the question, answer with a brief and precise answer derived from the contents.\n",
    "                Reply in the format:\n",
    "                \"answer\"\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"How big is the ANTISCAM dataset? \"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"220 human-human dialogs\"},          \n",
    "        { \"role\": \"user\", \"content\": f\"Contents of the research paper:{contents}\"},\n",
    "        { \"role\": \"user\", \"content\": f\"Question related to the paper:{question}\"}\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prompt_2(contents,question):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a question-answering system.\n",
    "                The user will provide you with the contents of a research paper.\n",
    "                The user will also provide a question to be answered.\n",
    "                Answer the question using the contents provided by the user.\n",
    "                Reply in the format:\n",
    "                \"answer\"\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"How big is the ANTISCAM dataset? \"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"220 human-human dialogs\"},         \n",
    "        { \"role\": \"user\", \"content\": f\"Contents of the research paper:{contents}\"},\n",
    "        { \"role\": \"user\", \"content\": f\"Question related to the paper:{question}\"}\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prompt_3(contents,question):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                The user will provide you with the contents of a research paper and a question.\n",
    "                Answer the question based on the contents in 1 sentence.\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"How big is the ANTISCAM dataset? \"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"220 human-human dialogs\"},           \n",
    "        { \"role\": \"user\", \"content\": f\"Contents of the research paper:{contents}\"},\n",
    "        { \"role\": \"user\", \"content\": f\"Question related to the paper:{question}\"}\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prompt_4(contents,question):\n",
    "    messages = [\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "                You are a child who has to answer a question with no knowledge of the domain.\n",
    "                The user will provide you with the contents of a research paper.\n",
    "                The user will also provide a question to be answered.\n",
    "                Strictly answer in one sentence or less. \n",
    "                The answer should strictly be an exact extracted text from the contents.\n",
    "                If an exact extracted text does not answer the question, answer like a child reading the contents.\n",
    "                Reply in the format:\n",
    "                \"answer\"\n",
    "            \"\"\"\n",
    "        },\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.\"},\n",
    "        { \"role\": \"system\", \"name\": \"example_user\", \"content\":\"How big is the ANTISCAM dataset? \"},\n",
    "        { \"role\": \"system\", \"name\": \"example_system\", \"content\": \"220 human-human dialogs\"},          \n",
    "        { \"role\": \"user\", \"content\": f\"Contents of the research paper:{contents}\"},\n",
    "        { \"role\": \"user\", \"content\": f\"Question related to the paper:{question}\"}\n",
    "    ]\n",
    "    events = call_gpt(messages)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50\n",
      "220 human-human dialogs.\n",
      "We design a hierarchical intent annotation scheme that separates on-task and off-task information, where on-task intents are task-specific actions and off-task intents are general dialog acts that convey syntax information.\n",
      "TransferTransfo and hybrid model.\n",
      "Automatic Evaluation Metrics: Perplexity, Response-Intent Prediction (RIP), Response-Slot Prediction (RSP), Extended Response-Intent Prediction (ERIP), Extended Response-Slot Prediction (ERSP).\n",
      "Human Evaluation Metrics: Fluency, Coherence, Engagement, Dialog length, Task Success Score.\n",
      "1/50\n",
      "We illustrate these differences by building a simple entity linker, OpenTapioca, which only uses data from Wikidata, and show that it is competitive with other systems with access to larger data sources for some tasks.\n",
      "2/50\n",
      "Liu et. al (2015) and Yang et. al (2012)\n",
      "Detection Error Trade-off (DET) curves.\n",
      "No, the paper does not mention whether their methods are fully supervised or not.\n",
      "Yes, they built a dataset of rumors consisting of 202 confirmed rumors.\n",
      "Chinese\n",
      "The Cambridge dictionary defines a rumor as information of doubtful or unconfirmed truth.\n",
      "3/50\n",
      "LDA, Doc-NADE, HTMM, and GMNTM.\n",
      "generative model evaluation and document classification\n",
      "4/50\n",
      "CoNLL2003, OntoNotes 5.0, OntoNotes 4.0 (Chinese part), MSRA, Weibo NER, and Resume NER.\n",
      "By using relative positional encoding ($R_{t-j}$) in the attention score calculation, the attention mechanism in the Transformer can distinguish different directions and distances between tokens.\n",
      "In this paper, the proposed TENER model outperforms previous models in the six NER datasets, achieving state-of-the-art performance without considering pre-trained language models or designed features.\n",
      "5/50\n",
      "KALM achieves an accuracy of 95.6% and KALM-QA achieves an accuracy of 95% for parsing the queries.\n",
      "SEMAFOR, SLING, and Stanford KBP system.\n",
      "MetaQA dataset BIBREF14.\n",
      "6/50\n",
      "i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach; ii.) an investigation on the quality of existing Italian word embeddings for this task; iii.) a comparison against a state-of-the-art discrete classifier.\n",
      "FBK-HLT BIBREF23 is the baseline system used in this paper.\n",
      "7/50\n",
      "Candidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.\n",
      "First, they trained domain-specific word embeddings using the Word2Vec CBOW model and then used k-means clustering to cluster the embeddings of the gender-associated words.\n",
      "over 300K sentences\n",
      "The centroid of the cluster was used as a strong baseline label.\n",
      "8/50\n",
      "Data-driven models usually respond to abuse by ranking low in general and often producing responses that can be interpreted as flirtatious.\n",
      "9960 HITs from 472 crowd workers.\n",
      "14 response types\n",
      "9/50\n",
      "10/50\n",
      "Turkish, Finnish, Czech, German, Spanish, Catalan, and English.\n",
      "gold morphological features\n",
      "11/50\n",
      "The Semantic Scholar corpus, Springer Nature's SciGraph, The Textbook Question Answering corpus, Wikipedia, Flickr30K, and COCO.\n",
      "The captions in the paper are in English.\n",
      "The paper explores the ad-hoc approaches of HolE, Vecsigrafo, Embedding network, 2WayNet, VSE++, and DSVE-loc.\n",
      "The supervised baselines they compared with are the direct combination baseline and the supervised pre-training baseline.\n",
      "From the Semantic Scholar corpus and Springer Nature's SciGraph, they extracted figures and captions from scientific publications.\n",
      "12/50\n",
      "Weka baseline BIBREF5.\n",
      "\"Our intra-sentence attention RNN was able to outperform the Weka baseline on the development dataset, but performed worse than the baseline on the test dataset, indicating a lack of generalization.\"\n",
      "training, validation and test datasets provided for the shared task BIBREF5\n",
      "13/50\n",
      "Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it.\n",
      "English, French, and Arabic.\n",
      "(a) directness or indirectness of the text, (b) offensiveness, disrespectfulness, hatefulness, fear out of ignorance, abusiveness, or normalcy, (c) the attribute based on which it discriminates against an individual or a group of people, (d) the name of the discriminated group, and (e) the sentiment of the annotators towards the content.\n",
      "The final dataset is composed of a pilot corpus of 100 tweets per language, and comparable corpora of 5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets.\n",
      "14/50\n",
      "SWT may be used for semantic disambiguation in MT, support the translation of named entities, address the non-standard language problem, translate knowledge bases, and adapt the MT system to user characteristics.\n",
      "The challenges associated with the use of Semantic Web technologies in Machine Translation include the lack of large bilingual datasets for non-European languages, the poor run-time performance of reasoners, the disambiguation of named entities, the handling of non-standard speech and idioms, and the need to address reordering errors and lexical and syntactic ambiguity.\n",
      "The paper does not mention any other obstacles to automatic translations besides the ones mentioned in the abstract.\n",
      "15/50\n",
      "root mean square, zero crossing rate, moving window average, kurtosis, and power spectral entropy\n",
      "USC-TIMIT database (for data set A) and a dataset generated in the experiment (for data set B).\n",
      "16/50\n",
      "sentiment classification (SST-2), natural language inference (MNLI-m, MNLI-mm, QNLI) and pairwise semantic equivalence (QQP)\n",
      "17/50\n",
      "news articles and corresponding highlights from the DailyMail website.\n",
      "They use the DailyMail news highlights corpus.\n",
      "18/50\n",
      "This background knowledge is crucial for solving many difficult, ambiguous natural language problems in coreference resolution and question answering, as well as the creation of other reasoning machines.\n",
      "Intrinsic geometry of spaces of learned representations refers to the inherent structure and relationships within the embedding space that are automatically enforced during the learning process, resulting in globally consistent structured predictions of the ontology.\n",
      "19/50\n",
      "Yes, the paper mentions that they pre-trained their models on the Stanford Sentiment Treebank, which is a sentiment corpus.\n",
      "We extracted word unigrams and bigrams, and used term frequencies (TF) and Inverse document-frequency (IDF) to transform these features. We also included word embeddings pretrained on large corpora, sentiment features from manually constructed lexica, and pre-trained sentiment embeddings.\n",
      "English and Spanish.\n",
      "20/50\n",
      "AI2D-RST contains three graphs: Grouping, Connectivity, and Discourse structure.\n",
      "The annotation for AI2D was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk.\n",
      "Expert annotations and crowd-sourced annotations are compared in terms of their representations of diagrammatic structures and the performance of various graph neural network architectures in classifying nodes from both resources.\n",
      "Amazon Mechanical Turk\n",
      "Two expert annotators who have linguistic training.\n",
      "21/50\n",
      "HAN (Hierarchical Attention Network) and CNN (Convolutional Neural Network).\n",
      "The dataset they use is based on Wikipedia crawl data, specifically using the \"List of Controversial articles\" overview page of 2018 and 2009 as a seed set of controversial articles.\n",
      "lexical methods trained on common (e.g. fulltext) features\n",
      "22/50\n",
      "64M segments from YouTube videos.\n",
      "64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens.\n",
      "The features derived from the videos are 1500-dimensional vectors, extracted from the video frames at 1-second intervals.\n",
      "23/50\n",
      "\"an established NMT architecture based on LSTMs and implementing the attention mechanism.\"\n",
      "Symbolic rewriting is the process of transforming one symbolic expression into another using a set of predefined rules or equations.\n",
      "24/50\n",
      "anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors.\n",
      "20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set.\n",
      "25/50\n",
      "F-Score Driven Model II achieves a new state-of-the-art NER system in Chinese social media.\n",
      "Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media.\n",
      "Sina Weibo service in China.\n",
      "A modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media.\n",
      "26/50\n",
      "NLI (XNLI dataset), document classification (MLDoc dataset), intent classification, POS tagging, and NER.\n",
      "MMTE improves upon the bilingual baseline on low resource language pairs, but performs worse than the bilingual baseline on high resource language pairs.\n",
      "These languages were chosen based on the intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by their mNMT model.\n",
      "27/50\n",
      "We evaluate the dialogue using both automatic evaluation metrics such as perplexity and BLEU, as well as human evaluation where annotators compare dialogues based on engagingness, interestingness, and humanness.\n",
      "We observe a significant gap between the cross-lingual model and other models.\n",
      "We adapt the Poly-encoder model by using the Google Translate API to translate target languages query to English as the input to the model, then translate the English response back to the target language.\n",
      "Chinese, French, Indonesian, Italian, Korean, and Japanese.\n",
      "28/50\n",
      "We automatically create a test set from the OpenSubtitles corpus.\n",
      "Our baseline models include a standard bidirectional RNN model with attention, a concatenation model where each sentence is concatenated with the preceding sentence, a multi-encoder architecture with hierarchical attention, and a context-agnostic Transformer.\n",
      "s-hier, s-t-hier, s-hier-to-2, concat21, BIBREF8\n",
      "English and German.\n",
      "29/50\n",
      "In this paper, they use a simple entity linking algorithm that directly links the mention to the entity with the greatest commonness score, which is calculated based on the anchor links in Wikipedia.\n",
      "An MLP (Multi-Layer Perceptron) with three dense layers.\n",
      "FIGER (GOLD) BIBREF0 and BBN BIBREF5.\n",
      "30/50\n",
      "We recruited one domain expert to annotate the sentences in the dataset.\n",
      "The model's F1-score is used for evaluation.\n",
      "The model gives an F1-score of 0.89 for the concept recognition task.\n",
      "abb: represents abbreviations such as TRL representing Technology Readiness Level.\n",
      "grp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.\n",
      "syscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.\n",
      "opcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.\n",
      "seterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.\n",
      "event: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.\n",
      "org: represents an organization such as `NASA', `aerospace industry', etc.\n",
      "art: represents names of artifacts or instruments such as `AS1300'\n",
      "cardinal: represents numerical values such as `1', `100', 'one' etc.\n",
      "loc: represents location-like entities such as component facilities or centralized facility.\n",
      "mea: represents measures, features, or behaviors such as cost, risk, or feasibility.\n",
      "BERT BIBREF16\n",
      "3700 sentences.\n",
      "31/50\n",
      "precision, recall, and F-measure\n",
      "The hand-crafted features used are the position of the sentence, sentence length, and tense.\n",
      "The word embeddings used in the study are specific word embeddings trained on a specific corpus, inspired by Tang et al.'s Sentiment-Specific Word Embedding model.\n",
      "Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors, and specific word vectors.\n",
      "Argumentative zoning is a process of assigning rhetorical status to sentences in scientific articles, which helps provide readers with a general discourse context to better understand, compare, and analyze scientific ideas.\n",
      "32/50\n",
      "We used Twitter's REST API to collect the tweet objects of these tweets.\n",
      "Spam Detection 1: We use the Twitter spam detection method proposed by Wu et al. BIBREF4 . It uses the Word2Vec and Doc2Vec models to encode the tweets into a vector representation, which is fed to a MLP classifier in order to classify the tweets as spam or not-spam. We use the same methodology to classify tweets in our dataset as blackmarket or genuine. Spam Detection 2: For baseline 2, we consider the approach proposed by Rajdev et. al. BIBREF11 . They proposed flat and hierarchical classifications approaches with few of the standard set of features which can classify spam, fake and legitimate tweets. We use their experimental setup with Random Forest classifier on our dataset.\n",
      "English\n",
      "YouLikeHits and Like4Like.\n",
      "33/50\n",
      "English, French, and Chinese.\n",
      "6-layer decoder.\n",
      "10-layer encoder.\n",
      "CorefNqg, Mp-Gsn, and Xlm are the baselines used in the research paper.\n",
      "34/50\n",
      "Human judges prefer stories that were generated hierarchically by first creating a premise and creating a full story based on it with a seq2seq model.\n",
      "Perplexity and prompt ranking accuracy.\n",
      "Language Models, seq2seq models, Ensemble models, and KNN models.\n",
      "The convolutional language model from BIBREF4 is used to generate the premise.\n",
      "WritingPrompts forum.\n",
      "35/50\n",
      "word2vec, fastText, GloVe, Baroni, SL999.\n",
      "SICK, STSB, MRPC, SICK-R, and SICK-E.\n",
      "HCTI BIBREF5, HTCI, and InferSent BIBREF23.\n",
      "36/50\n",
      "Fleiss's Kappa for inter-annotator agreement.\n",
      "170 workers\n",
      "Neighborhood overlap, downward overlap, upward overlap, inward overlap, outward overlap, and bidirectional overlap.\n",
      "Aggressive language, Repetition, Harmful intent, Visibility among peers, Power imbalance.\n",
      "Aggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive. The user either addresses a group or individual, and the message contains at least one phrase that could be described as confrontational, derogatory, insulting, threatening, hostile, violent, hateful, or sexually abusive. Repetition: (rep) The target user has received at least two aggressive messages in total (either from the author or from another user in the visible thread). Harmful intent: (harm) The tweet was designed to tear down or disadvantage the target user by causing them distress or by harming their public image. The target does not respond agreeably as to a joke or an otherwise lighthearted comment. Visibility among peers: (peer) At least one other user besides the target has liked, retweeted, or responded to at least one of the author's messages. Power imbalance: (power) Power is derived from authority and perceived social advantage. Celebrities and public figures are more powerful than common users. Minorities and disadvantaged groups have less power. Bullies can also derive power from peer support.\n",
      "37/50\n",
      "We evaluated the similarity of the generated texts with training data objectively and the humor content subjectively.\n",
      "CrowdTruth and Subreddits.\n",
      "inspirational quotes\n",
      "38/50\n",
      "Multiple filters of the same shape.\n",
      "25.1% relative improvement in MRR and 1% absolute improvement in Hits@10.\n",
      "39/50\n",
      "BERT, BERT with Bi-LSTM, DenseNet, HighwayLSTM, BIMPM, Sim-Transformer\n",
      "CoNLL03 dataset, Yahoo Answer Classification Dataset, and \"Quora-Question-Pair\" dataset.\n",
      "40/50\n",
      "0.874 averaged weighted kappa value.\n",
      "Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication.\n",
      "933 manually identified adpositions.\n",
      "41/50\n",
      "Fake News Dataset\n",
      "SVM, Logistic Regression, ANN, LSTM, and Random Forest.\n",
      "42/50\n",
      "Ablation study to explore the importance of different feature families.\n",
      "achieving 98.2% in the test data\n",
      "gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.\n",
      "further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.\n",
      "43/50\n",
      "The sentence alignment quality is evaluated according to a 5-point scale used in KocabiyikogluETAL:18, which includes categories such as wrong alignment, partial alignment with slightly compositional translational equivalence, partial alignment with compositional translation and additional or missing information, correct alignment with compositional translation and few additional or missing information, and correct alignment and fully compositional translation.\n",
      "The evaluation of the audio-text alignment quality was conducted according to a 3-point scale: wrong alignment, partial alignment with some missing words or sentences, and correct alignment allowing non-spoken syllables at the start or end.\n",
      "44/50\n",
      "10.3% and 5.3%\n",
      "45/50\n",
      "Our method outperforms existing methods in terms of performance, but the exact improvement is not mentioned in the contents of the research paper.\n",
      "standard SICK (Sentences Involving Compositional Knowledge) dataset\n",
      "46/50\n",
      "SRE18 development and SRE18 evaluation datasets.\n",
      "Table 8. Subsystem performance on SRE18 DEV and EVAL set.\n",
      "47/50\n",
      "Yes, they reduce language variation of text by enhancing frequencies of semantically related words in the similarity sets.\n",
      "economic and political domains\n",
      "WordNet, EuroVoc, and RuThes.\n",
      "48/50\n",
      "\"Drawing upon these definitions, we define hate speech as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\"\n",
      "logistic regression with L2 regularization\n",
      "33,458 Twitter users.\n",
      "85.4 million tweets.\n",
      "49/50\n",
      "On the SST-2 sentiment classification dataset, not fine-tuning all of the layers leads to improved quality.\n",
      "No, they only test against the base variant of BERT and RoBERTa.\n"
     ]
    }
   ],
   "source": [
    "for i,datum in enumerate(data_reduced):\n",
    "    print('{}/{}'.format(i, len(data_reduced)))\n",
    "    # example_content = \"To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.\"\n",
    "    for qa in datum['qas']:\n",
    "        contents = qa['evidence']\n",
    "        LLM_answer = Prompt_4(contents,qa['question'])\n",
    "        print(LLM_answer)\n",
    "        qa['GPT_Answer'] = LLM_answer\n",
    "save_json(data_reduced, r'Data\\responses_P4_fs.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt used for openAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(datum_sentences):\n",
    "    sentence_list = [\" \".join(sentence_word_list) for sentence_word_list in datum_sentences] \n",
    "    paragraph = \" \".join(sentence_list)\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punctuation_with_whitespace(input_string):\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    return input_string.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = json.load(open(r\"Data\\responses_P4_fs.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryam\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryam\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\aryam\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/50\n",
      "3/50\n",
      "4/50\n",
      "5/50\n",
      "6/50\n",
      "7/50\n",
      "8/50\n",
      "9/50\n",
      "10/50\n",
      "11/50\n",
      "12/50\n",
      "13/50\n",
      "14/50\n",
      "15/50\n",
      "16/50\n",
      "17/50\n",
      "18/50\n",
      "19/50\n",
      "20/50\n",
      "21/50\n",
      "22/50\n",
      "23/50\n",
      "24/50\n",
      "25/50\n",
      "26/50\n",
      "27/50\n",
      "28/50\n",
      "29/50\n",
      "30/50\n",
      "31/50\n",
      "32/50\n",
      "33/50\n",
      "34/50\n",
      "35/50\n",
      "36/50\n",
      "37/50\n",
      "38/50\n",
      "39/50\n",
      "40/50\n",
      "41/50\n",
      "42/50\n",
      "43/50\n",
      "44/50\n",
      "45/50\n",
      "46/50\n",
      "47/50\n",
      "48/50\n",
      "49/50\n"
     ]
    }
   ],
   "source": [
    "scorer = Scorers()\n",
    "rel = st.relevance()\n",
    "fl = st.fluency()\n",
    "cr = st.Correctness()\n",
    "for i,datum in enumerate(data_final):\n",
    "    print('{}/{}'.format(i, len(data_final)))\n",
    "    for qa in datum['qas']:\n",
    "        bert_scores = []\n",
    "        ner_overlap = []\n",
    "        writer_summ = qa['answers']\n",
    "        llm_summ = qa['GPT_Answer']\n",
    "        readability = st.Readability(llm_summ)\n",
    "        formality = st.Formality(llm_summ)\n",
    "        bleu_1,bleu_2,bleu_3,bleu_4 = scorer.compute_bleu(writer_summ,llm_summ)\n",
    "        \n",
    "        qa['bleu_1'] = bleu_1\n",
    "        qa['bleu_2'] = bleu_2\n",
    "        qa['bleu_3'] = bleu_3\n",
    "        qa['bleu_4'] = bleu_4\n",
    "\n",
    "        rogue = scorer.compute_rouge(writer_summ,llm_summ)\n",
    "        qa['rogue'] = rogue\n",
    "\n",
    "        meteor = scorer.compute_meteor(writer_summ,llm_summ)\n",
    "        qa['meteor'] = meteor\n",
    "        qa['QA_Relevance'] = rel.default(qa['question'],qa['GPT_Answer'])\n",
    "        qa['CA_Relevance'] = rel.default(qa['evidence'],qa['GPT_Answer'])\n",
    "        qa['fluency'] = fl.default(qa['GPT_Answer'])\n",
    "        for ref in writer_summ:\n",
    "        #     # bert_scores.append(scorer.Bert_Score([ref],[llm_summ]))\n",
    "            ner_overlap.append(cr.default(ref,qa['GPT_Answer']))\n",
    "        qa['Correctness'] = max(ner_overlap)\n",
    "        # qa['bert_score'] = max(bert_scores)\n",
    "        qa['readability_LLM'] = readability.default()\n",
    "        qa['formality_LLM'] = formality.default()\n",
    "\n",
    "save_json(data_final, r'Data/scores_P4_fs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50\n",
      "1/50\n",
      "2/50\n",
      "3/50\n",
      "4/50\n",
      "5/50\n",
      "6/50\n",
      "7/50\n",
      "8/50\n",
      "9/50\n",
      "10/50\n",
      "11/50\n",
      "12/50\n",
      "13/50\n",
      "14/50\n",
      "15/50\n",
      "16/50\n",
      "17/50\n",
      "18/50\n",
      "19/50\n",
      "20/50\n",
      "21/50\n",
      "22/50\n",
      "23/50\n",
      "24/50\n",
      "25/50\n",
      "26/50\n",
      "27/50\n",
      "28/50\n",
      "29/50\n",
      "30/50\n",
      "31/50\n",
      "32/50\n",
      "33/50\n",
      "34/50\n",
      "35/50\n",
      "36/50\n",
      "37/50\n",
      "38/50\n",
      "39/50\n",
      "40/50\n",
      "41/50\n",
      "42/50\n",
      "43/50\n",
      "44/50\n",
      "45/50\n",
      "46/50\n",
      "47/50\n",
      "48/50\n",
      "49/50\n"
     ]
    }
   ],
   "source": [
    "temp = json.load(open(r\"Data\\scores_P4_fs.json\"))\n",
    "max_rogue_1_f = 0.0\n",
    "max_rogue_2_f = 0.0\n",
    "max_rogue_l_f = 0.0\n",
    "for i,datum in enumerate(temp):\n",
    "    print('{}/{}'.format(i, len(data_final)))\n",
    "    for qa in datum['qas']:\n",
    "        for rogue_list in qa[\"rogue\"]:\n",
    "            for rogue_dict in rogue_list:\n",
    "                max_rogue_1_f = max(max_rogue_1_f, rogue_dict[\"rouge-1\"][\"f\"])\n",
    "                max_rogue_2_f = max(max_rogue_2_f, rogue_dict[\"rouge-2\"][\"f\"])\n",
    "                max_rogue_l_f = max(max_rogue_l_f, rogue_dict[\"rouge-l\"][\"f\"])\n",
    "        qa['rogue'] = {\"rogue-1\":max_rogue_1_f,\"rogue-2\":max_rogue_2_f,\"rogue-l\":max_rogue_l_f}\n",
    "save_json(temp, r'Data/results/scores_P4_fs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_metrics(data):\n",
    "    meteor, bleu_1, bleu_2, bleu_3, bleu_4, rogue_1, rogue_2, rogue_l, meteor, QA_Relevance, CA_Relevance, fluency, Correctness, formality, readability_LLM = [], [], [], [], [], [], [], [], [], [], [], [], [], [], []    # bleu_1=[]\n",
    "    # bleu_2=[]\n",
    "    # bleu_3=[]\n",
    "    # bleu_4 =[]\n",
    "    # rogue_1=0.0\n",
    "    # rogue_2=0.0\n",
    "    # rogue_l=0.0\n",
    "    # meteor=0.0\n",
    "    # QA_Relevance=0.0\n",
    "    # CA_Relevance=0.0\n",
    "    # fluency=0.0\n",
    "    # Correctness=0.0\n",
    "    # formality=0.0\n",
    "    # readability_LLM=0.0\n",
    "    for i,datum in enumerate(data):\n",
    "        print('{}/{}'.format(i, len(data_final)))\n",
    "        for qa in datum['qas']:\n",
    "            bleu_1.append(qa[\"bleu_1\"])\n",
    "            bleu_2.append(qa[\"bleu_2\"])\n",
    "            bleu_3.append(qa[\"bleu_3\"])\n",
    "            bleu_4.append(qa[\"bleu_4\"])\n",
    "            rogue_1.append(qa[\"rogue\"][\"rogue-1\"])\n",
    "            rogue_2.append(qa[\"rogue\"][\"rogue-2\"])\n",
    "            rogue_l.append(qa[\"rogue\"][\"rogue-l\"])\n",
    "            meteor.append(qa[\"meteor\"])\n",
    "            QA_Relevance.append(qa[\"QA_Relevance\"])\n",
    "            CA_Relevance.append(qa[\"CA_Relevance\"])\n",
    "            fluency.append(qa[\"fluency\"])\n",
    "            Correctness.append(qa[\"Correctness\"])\n",
    "            if qa[\"readability_LLM\"]<0.0:\n",
    "                readability_LLM.append(-qa[\"readability_LLM\"])\n",
    "            else:\n",
    "                readability_LLM.append(qa[\"readability_LLM\"])\n",
    "            if qa[\"formality_LLM\"]<0.0:\n",
    "                formality.append(-qa[\"formality_LLM\"])\n",
    "            else:\n",
    "                formality.append(qa[\"formality_LLM\"])\n",
    "    return {\n",
    "        \"meteor\":meteor,\"bleu_1\":bleu_1,\"bleu_2\":bleu_2,\"bleu_3\":bleu_3,\"bleu_4\":bleu_4,\"rogue_1\":rogue_1,\"rogue_2\":rogue_2,\"rogue_l\":rogue_l,\"meteor\":meteor,\"QA_Relevance\":QA_Relevance,\"CA_Relevance\":CA_Relevance,\"fluency\":fluency,\"Correctness\":Correctness,\"formality\":formality,\"readability_LLM\":readability_LLM\n",
    "        }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50\n",
      "1/50\n",
      "2/50\n",
      "3/50\n",
      "4/50\n",
      "5/50\n",
      "6/50\n",
      "7/50\n",
      "8/50\n",
      "9/50\n",
      "10/50\n",
      "11/50\n",
      "12/50\n",
      "13/50\n",
      "14/50\n",
      "15/50\n",
      "16/50\n",
      "17/50\n",
      "18/50\n",
      "19/50\n",
      "20/50\n",
      "21/50\n",
      "22/50\n",
      "23/50\n",
      "24/50\n",
      "25/50\n",
      "26/50\n",
      "27/50\n",
      "28/50\n",
      "29/50\n",
      "30/50\n",
      "31/50\n",
      "32/50\n",
      "33/50\n",
      "34/50\n",
      "35/50\n",
      "36/50\n",
      "37/50\n",
      "38/50\n",
      "39/50\n",
      "40/50\n",
      "41/50\n",
      "42/50\n",
      "43/50\n",
      "44/50\n",
      "45/50\n",
      "46/50\n",
      "47/50\n",
      "48/50\n",
      "49/50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'meteor': 0.34805058240299,\n",
       " 'bleu_1': 0.2006544588887659,\n",
       " 'bleu_2': 0.1289090567154082,\n",
       " 'bleu_3': 0.09343321184011455,\n",
       " 'bleu_4': 0.06989125744336004,\n",
       " 'rogue_1': 0.7176462690305503,\n",
       " 'rogue_2': 0.6247442763566398,\n",
       " 'rogue_l': 0.6312824146905583,\n",
       " 'QA_Relevance': 0.546719310376303,\n",
       " 'CA_Relevance': 0.4752787411408989,\n",
       " 'fluency': 72.6482569983131,\n",
       " 'Correctness': 0.7219298245614036,\n",
       " 'formality': 52.30261809316837,\n",
       " 'readability_LLM': 51.49842105263158}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(r\"Data\\results\\LLama_P4_fs.json\"))\n",
    "res = dataset_metrics(data)\n",
    "for keys,values in res.items():\n",
    "    res[keys] = np.mean(values)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(r\"Data\\results\\LLama_P4.json\"))\n",
    "res = dataset_metrics(data)\n",
    "for keys,values in res.items():\n",
    "    res[keys] = np.mean(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
